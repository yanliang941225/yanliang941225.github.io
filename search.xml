<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[7.hive自定义一个jsonUDF]]></title>
    <url>%2F2018%2F09%2F29%2F7.hive%E8%87%AA%E5%AE%9A%E4%B9%89%E4%B8%80%E4%B8%AAjsonUDF%2F</url>
    <content type="text"><![CDATA[[TOC] 1.编辑解析jar1.1创建项目创建一个maven项目:pom.xml如些 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.codingkuai&lt;/groupId&gt; &lt;artifactId&gt;parseJsonHive&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-exec&lt;/artifactId&gt; &lt;version&gt;1.2.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;jdk.tools&lt;/groupId&gt; &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt; &lt;version&gt;1.6&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;$&#123;JAVA_HOME&#125;/lib/tools.jar&lt;/systemPath&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;filters&gt; &lt;filter&gt; &lt;artifact&gt;*:*&lt;/artifact&gt; &lt;excludes&gt; &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt; &lt;/excludes&gt; &lt;/filter&gt; &lt;/filters&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 1.2编写业务逻辑123456789101112131415161718192021222324252627282930313233343536package com.codingkuai;public class MovieRate &#123; private String movie; private String rate; private String timeStamp; private String uid; public String getMovie() &#123; return movie; &#125; public void setMovie(String movie) &#123; this.movie = movie; &#125; public String getRate() &#123; return rate; &#125; public void setRate(String rate) &#123; this.rate = rate; &#125; public String getTimeStamp() &#123; return timeStamp; &#125; public void setTimeStamp(String timeStamp) &#123; this.timeStamp = timeStamp; &#125; public String getUid() &#123; return uid; &#125; public void setUid(String uid) &#123; this.uid = uid; &#125; @Override public String toString() &#123; return movie + "\t" + rate + "\t" + timeStamp + "\t" + uid; &#125;&#125; 123456789101112131415161718package com.codingkuai;import org.apache.hadoop.hive.ql.exec.UDF;import parquet.org.codehaus.jackson.map.ObjectMapper;public class ParseJsonUDF extends UDF &#123; public String evaluate(String jsonLine) &#123; ObjectMapper objectMapper = new ObjectMapper(); try &#123; MovieRate readValue = objectMapper.readValue(jsonLine, MovieRate.class); return readValue.toString(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return ""; &#125;&#125; 1.3打包2.创建临时函数1231.把程序打包放到目标机器上去2.进入hive客户端，添加jar包：hive&gt;add jar /opt/parseJson.jar3.创建临时函数：create temporary function parsejson as 'com.codingkuai.ParseJsonUDF'; 3.测试12345678910111213创建存原始数据的表create table rat_json(line string) row format delimited;把json上传上去load data local inpath '/root/rating.json' into table rat_json;创建存解析数据后的表create table t_rating(movieid string,rate int,timestring string,uid string)row format delimited fields terminated by '\t';使用我们自定义的函数，把rat_json解析后插入到t_rating内insert overwrite table t_ratingselect split(parsejson(line),'\t')[0]as movieid,split(parsejson(line),'\t')[1] as rate,split(parsejson(line),'\t')[2] as timestring,split(parsejson(line),'\t')[3] as uid from rat_json;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hive</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[6.hive基本操作]]></title>
    <url>%2F2018%2F09%2F28%2F6.hive%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[[TOC] hive创建表以及如何加载数据到hive表中建表语法12345678910111213141516171819202122232425262728CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name [(col_name data_type [COMMENT col_comment], ...)] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [ROW FORMAT row_format] [STORED AS file_format] [LOCATION hdfs_path]说明：1、 CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。2、 EXTERNAL关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION），Hive 创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。3、 LIKE 允许用户复制现有的表结构，但是不复制数据。4、 ROW FORMAT DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]用户在建表的时候可以自定义 SerDe 或者使用自带的 SerDe。如果没有指定 ROW FORMAT 或者 ROW FORMAT DELIMITED，将会使用自带的 SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的 SerDe，Hive通过 SerDe 确定表的具体的列的数据。5、 STORED AS SEQUENCEFILE|TEXTFILE|RCFILE如果文件数据是纯文本，可以使用 STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。6、CLUSTERED BY对于每一个表（table）或者分区， Hive可以进一步组织成桶，也就是说桶是更为细粒度的数据范围划分。Hive也是 针对某一列进行桶的组织。Hive采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中。 把表（或者分区）组织成桶（Bucket）有两个理由：（1）获得更高的查询处理效率。桶为表加上了额外的结构，Hive 在处理有些查询时能利用这个结构。具体而言，连接两个在（包含连接列的）相同列上划分了桶的表，可以使用 Map 端连接 （Map-side join）高效的实现。比如JOIN操作。对于JOIN操作两个表有一个相同的列，如果对这两个表都进行了桶操作。那么将保存相同列值的桶进行JOIN操作就可以，可以大大较少JOIN的数据量。（2）使取样（sampling）更高效。在处理大规模数据集时，在开发和修改查询的阶段，如果能在数据集的一小部分数据上试运行查询，会带来很多方便。 具体实施123create table student(id int, name string, age int) row formate delimitedfields terminated by ','; 数据导入1234567方式一：直接把文件上传到/user/hive/warehouse/数据库名.db/表名/ hadoop fs -put /opt/student.txt /user/hive/warehouse/db1.db/student方式二: 通过load 本地文件load到表 load data local inpath &apos;/opt/student.txt&apos; into table student; hdfs内load到表 hdfs原路径下的文件不存在。 load data inpath &apos;/student.txt&apos; into table student; 修改表增加/删除分区必须是分区表才能进行本次操作 1234create table t_patition(ip string, duration int)partitioned by(country string)row format delimitedfields terminated by &apos;,&apos;; 1load data local inpath '/opt/partition.txt' into table t_patition partition(country="china"); 12添加分区alter table t_patition add partition(country="Japan");]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hive</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[4.mapreduce程序编写]]></title>
    <url>%2F2018%2F09%2F28%2F4.mapreduce%E7%A8%8B%E5%BA%8F%E7%BC%96%E5%86%99%2F</url>
    <content type="text"><![CDATA[MAPREDUCE 原理Mapreduce 是一个分布式运算程序的*编程框架，是用户开发“基于hadoop**的数据分析应用”的核心框架； Mapreduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个hadoop集群上 为什么要MAPREDUCE（1）海量数据在单机上处理因为硬件资源限制，无法胜任（2）而一旦将单机版程序扩展到集群来分布式运行，将极大增加程序的复杂度和开发难度（3）引入mapreduce框架后，开发人员可以将绝大部分工作集中在业务逻辑的开发上，而将分布式计算中的复杂性交由框架来处理 流程解析1、 一个mr程序启动的时候，最先启动的是MRAppMaster，MRAppMaster启动后根据本次job的描述信息，计算出需要的maptask实例数量，然后向集群申请机器启动相应数量的maptask进程 2、 maptask进程启动之后，根据给定的数据切片范围进行数据处理，主体流程为：a) 利用客户指定的inputformat来获取RecordReader读取数据，形成输入KV对b) 将输入KV对传递给客户定义的map()方法，做逻辑运算，并将map()方法输出的KV对收集到缓存c) 将缓存中的KV对按照K分区排序后不断溢写到磁盘文件 3、 MRAppMaster监控到所有maptask进程任务完成之后，会根据客户指定的参数启动相应数量的reducetask进程，并告知reducetask进程要处理的数据范围（数据分区） 4、 Reducetask进程启动之后，根据MRAppMaster告知的待处理数据所在位置，从若干台maptask运行所在机器上获取到若干个maptask输出结果文件，并在本地进行重新归并排序，然后按照相同key的KV为一个组，调用客户定义的reduce()方法进行逻辑运算，并收集运算输出的结果KV，然后调用客户指定的outputformat将结果数据输出到外部存储 MAPREDUCE 实践MAPREDUCE 示例编写及编程规范（1）用户编写的程序分成三个部分：Mapper，Reducer，Driver(提交运行mr程序的客户端)（2）Mapper的输入数据是KV对的形式（KV的类型可自定义）（3）Mapper的输出数据是KV对的形式（KV的类型可自定义）（4）Mapper中的业务逻辑写在map()方法中（5）map()方法（maptask进程）对每一个&lt;K,V&gt;调用一次（6）Reducer的输入数据类型对应Mapper的输出数据类型，也是KV（7）Reducer的业务逻辑写在reduce()方法中（8）Reducetask进程对每一组相同k的&lt;k,v&gt;组调用一次reduce()方法（9）用户自定义的Mapper和Reducer都要继承各自的父类（10）整个程序需要一个Drvier来进行提交，提交的是一个描述了各种必要信息的job对象 单词统计实例定义一个mapper类1234567891011121314151617181920212223242526272829303132333435363738394041424344package cn.codingkuai.mapreduce;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;/** * Mapper&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt; 介紹 * KEYIN: 是值框架读取到的数据的key类型 * 在默认的读取数据组件InputFormat下，读取的key是一行文本的偏移量，所以key的流行是long类型的 * VALUEIN：指框架读取到数据的value类型 * 在默认的读取数据组件InputFormat下，读到的value就是一行文本的内容，所以value的类型是String的 * KEYOUT：是指用户自定义逻辑方法返回的数据中的key的类型 这个是由用户业务逻辑决定的。 * 在我们的单词统计当中，我们输出的是单词作为key，所以类型是String * VALUEOUT：是指用户自定义逻辑方法返回的数据中value的类型 这个是由用户业务逻辑决定的。 * 在我们的单词统计当中，我们输出的是单词数量作为value，所以类型是Integer * 但是，String, Long都是jdk中自带的数据类型，在序列化的时候，效率比较低，hadoop为了提高序列化效率，他就 自定义了一套数据类型。 * Long -&gt; LongWritable * String -&gt; Text * Integer -&gt; IntWritable * null -&gt; NullWritable * @author yanliang * */public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;&#123; /** * 这个map方法就是mapreduce程序中被主题程序MapTask所调用的用户业务逻辑方法 * MapTask会驱动我们的读取数据组件InputFormat去读取数据(KEYIN，VALUE),每读取一个（K， V）,他就会传入到这个用户写的map方法中去调用一次 * 在默认的inputFormat冲突中，此处的key就是一行的起始偏移量，value就是一行的内容 */ @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, IntWritable&gt;.Context context) throws IOException, InterruptedException &#123; String line = value.toString(); String[] split = line.split(" "); for (String word : split) &#123; context.write(new Text(word), new IntWritable(1)); &#125; &#125;&#125; 定义一个reducer类123456789101112131415161718192021222324252627282930313233package cn.codingkuai.mapreduce;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;/** * reducetask在调用我们的reduce方法 * reducetask应该接受到map阶段中所有maptask输出的数据中的一部分 * (key.hashcode % numReduceTask == 本ReduceTask编号) * * reducetask将接受到的kv数据拿来处理时，是这样调用我们的reduce方法的： * 先将自己接受到所有kv对按照K分组（根据K是否相同） * 然后将一组kv中的K传给我们的reduce方法的key变量，把这一组kv中所有v用一个迭代器传给reduce方法的变量values * @author yanliang * */public class WordCountReduce extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context) throws IOException, InterruptedException &#123; int count = 0; for (IntWritable v : values) &#123; int i = v.get(); count += i; &#125; context.write(key, new IntWritable(count)); &#125;&#125; 定义一个主类，用来描述job并提交job1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374package cn.codingkuai.mapreduce;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;/** * 本类是客户端用来指定wordCount job程序运行时候所需要的很多参数 比如：指定哪一个类左右map阶段的业务逻辑类 * 哪个类作为reduce阶段的业务逻辑类 指定用那个组件作为数据的读取组件 数据结果输出组件 指定这个wordcount jar包锁在的路径 * 以及其它各种所需要的参数 * * @author yanliang * */public class WordCountDriver &#123; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); System.setProperty("HADOOP_USER_NAME", "root"); conf.set("fs.defaultFS", "hdfs://min1:9000");// conf.set("mapreduce.framework.name", "yarn");// conf.set("yarn.resourcemanager.hostname", "min1"); Job job = Job.getInstance(conf); // 告诉框架，我们的程序所有jar包的位置.linux上的路径// job.setJar("/opt/temp/wordcount.jar"); job.setJarByClass(WordCountDriver.class); // 告诉框架，我们程序所用的mapper和reduce类是什么 job.setMapperClass(WordCountMapper.class); job.setReducerClass(WordCountReduce.class); // 告诉框架，我们程序输出的数据类型 job.setMapOutputKeyClass(Text.class); // map阶段 job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(Text.class); // 最终结果 job.setOutputValueClass(IntWritable.class); // 告诉框架，我们程序使用的数据读取组件 结果输出所用的组件 // TextInputFormat是mapreduce程序中内置的一种读取数据组件 准确的说叫做读取文件的输入组件 job.setInputFormatClass(TextInputFormat.class); job.setOutputFormatClass(TextOutputFormat.class); // 告诉框架，我们要处理的数据文件在哪个路径下 FileInputFormat.setInputPaths(job, new Path("/wordcount/input")); FileOutputFormat.setOutputPath(job, new Path("/wordcount/output")); boolean res = job.waitForCompletion(true); System.exit(res ? 0 : 1); // hadoop 上运行 //hadoop jar wordcount.jar cn.codingkuai.mapreduce.WordCountDriver // 插件结果 //hadoop fs -cat /wordcount/output/part-r-00000 /* * guo 1 hello 7 nihao 2 Re. 1 shi 1 tom 1 tom1 1 uu 1 wo 1 yanliang 1 zhong 1 zhongguo 1 */ &#125;&#125; MAPREDUCE程序运行模式本地运行模式（1）mapreduce程序是被提交给LocalJobRunner在本地以单进程的形式运行 （2）而处理的数据及输出结果可以在本地文件系统，也可以在hdfs上 （3）怎样实现本地运行？写一个程序，不要带集群的配置文件（本质是你的mr程序的conf中是否有mapreduce.framework.name=local以及yarn.resourcemanager.hostname参数） （4）本地模式非常便于进行业务逻辑的debug，只要在eclipse**中打断点即可 如果在windows下想运行本地模式来测试程序逻辑，需要在windows中配置环境变量： ％HADOOP_HOME**％ = d:/hadoop-2.6.1 %PATH% = ％HADOOP_HOME**％\bin 并且要将d:/hadoop-2.6.1的lib和bin目录替换成windows平台编译的版本 项目细节自定义对象实现MR中的序列化接口如果需要将自定义的bean放在key中传输，则还需要实现comparable接口，因为mapreduce框中的shuffle过程一定会对key进行排序,此时，自定义的bean实现的接口应该是： public class FlowBean implements WritableComparable 需要自己实现的方法是： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990package cn.codingkuai;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.WritableComparable;public class FlowBean implements WritableComparable&lt;FlowBean&gt;&#123; private long upFlow; private long downFlow; private long sumFlow; public FlowBean() &#123; &#125; public FlowBean(long upFlow, long downFlow) &#123; super(); this.upFlow = upFlow; this.downFlow = downFlow; this.sumFlow = upFlow + downFlow; &#125; public FlowBean(long upFlow, long downFlow, long sumFlow) &#123; super(); this.upFlow = upFlow; this.downFlow = downFlow; this.sumFlow = sumFlow; &#125; public void set(long upFlow, long downFlow) &#123; this.upFlow = upFlow; this.downFlow = downFlow; this.sumFlow = (upFlow + downFlow); &#125; public long getUpFlow() &#123; return upFlow; &#125; public void setUpFlow(long upFlow) &#123; this.upFlow = upFlow; &#125; public long getDownFlow() &#123; return downFlow; &#125; public void setDownFlow(long downFlow) &#123; this.downFlow = downFlow; &#125; public long getSumFlow() &#123; return sumFlow; &#125; public void setSumFlow(long sumFlow) &#123; this.sumFlow = sumFlow; &#125; /** * 序列化 */ public void write(DataOutput out) throws IOException &#123; out.writeLong(this.upFlow); out.writeLong(this.downFlow); out.writeLong(this.sumFlow); &#125; /** * 反序列化，顺序和序列化顺序一样 */ public void readFields(DataInput in) throws IOException &#123; this.upFlow = in.readLong(); this.downFlow = in.readLong(); this.sumFlow = in.readLong(); &#125; /** * TextOutputFormat组件输出结果时候调用的是toString方法 */ public String toString() &#123; return this.upFlow + "\t" + this.downFlow + "\t" + this.sumFlow; &#125; public int compareTo(FlowBean o) &#123; return (int) (o.getSumFlow() - this.getSumFlow()); &#125;&#125; 自定义partitioner如果我们通过key不同去到不通文件中的需求，这时候我们需要去设置ReduceTask个数默认是一个，其次去编写分片分规则。 123456789101112131415161718192021222324252627package cn.codingkuai;import java.util.HashMap;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Partitioner;public class ProvincePartitioner extends Partitioner&lt;Text, FlowBean&gt;&#123; static HashMap&lt;String, Integer&gt; provinceMap = new HashMap&lt;String, Integer&gt;(); static &#123; provinceMap.put("135", 0); provinceMap.put("136", 1); provinceMap.put("137", 2); provinceMap.put("138", 3); provinceMap.put("139", 4); &#125; @Override public int getPartition(Text key, FlowBean value, int numPartitions) &#123; Integer code = provinceMap.get(key.toString().substring(0, 3)); return code == null ? 5 : code; &#125;&#125; 12job.setPartitionerClass(ProvincePartitioner.class);job.setNumReduceTasks(6); MAPREDUCE中的Combiner（1）combiner是MR程序中Mapper和Reducer之外的一种组件 （2）combiner组件的父类就是Reducer （3）combiner和reducer的区别在于运行的位置： Combiner是在每一个maptask所在的节点运行 Reducer是接收全局所有Mapper的输出结果； (4) combiner的意义就是对每一个maptask的输出进行局部汇总，以减小网络传输量 具体实现步骤： 1、 自定义一个combiner继承Reducer，重写reduce方法 2、 在job中设置： job.setCombinerClass(CustomCombiner.class) (5) combiner能够应用的前提是不能影响最终的业务逻辑 而且，combiner的输出kv应该跟reducer的输入kv类型要对应起来]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>java</tag>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5.hive安装]]></title>
    <url>%2F2018%2F09%2F26%2F5.hive%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[Hive只在一个节点上安装即可 安装mysql1参考：http://www.cnblogs.com/starof/p/4680083.html 配置hive123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263（a）配置HIVE_HOME环境变量 vi conf/hive-env.sh 配置其中的$hadoop_home HADOOP_HOME=/opt/hadoop-2.6.4（b）配置元数据库信息 vi hive-site.xml 添加如下内容：&lt;configuration&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;&lt;value&gt;jdbc:mysql://127.0.0.1:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;&lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;&lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;&lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;&lt;value&gt;root&lt;/value&gt;&lt;description&gt;username to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;&lt;value&gt;963852&lt;/value&gt;&lt;description&gt;password to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;/configuration&gt;（C）Jline包版本不一致的问题，需要拷贝hive的lib目录中jline.2.12.jar的jar包替换掉hadoop中的 cp jline-2.12.jar /opt/hadoop-2.6.4/share/hadoop/yarn/lib/(d) 添加环境变量 vi /etc/profile export JAVA_HOME=/opt/jdk1.8 export HADOOP_HOME=/opt/hadoop-2.6.4 export HIVE_HOME=/opt/hive export PATH=$&#123;PATH&#125;:$&#123;JAVA_HOME&#125;/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HIVE_HOME/bin. /etc/profile (e） mysql驱动添加到lib目录下(f) 启动 hive hive也可以启动为一个服务器，来对外提供启动方式，启动为前台：bin/hiveserver2启动为后台：nohup bin/hiveserver2 1&gt;/var/log/hiveserver.log 2&gt;/var/log/hiveserver.err &amp;启动成功后，可以在别的节点上用beeline去连接方式（1）hive/bin/beeline 回车，进入beeline的命令界面输入命令连接hiveserver2beeline&gt; !connect jdbc:hive2://min3:10000（itcast01是hiveserver2所启动的那台主机名，端口默认是10000） 方式（2）或者启动就连接：bin/beeline -u jdbc:hive2://min3:10000 -n root接下来就可以做正常sql查询了]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hive</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2.hdfs-shell操作.md]]></title>
    <url>%2F2018%2F09%2F20%2F2.hdfs-shell%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[[TOC] 1.hdfs命令行客户端HDFS提供shell命令行客户端，使用方法如下： 可以使用一下两种形式： 12hadoop fs -… &lt;args&gt;hdfs dfs -… &lt;args&gt; 2.命令行客户端支持的命令参数123456789101112131415161718192021222324252627282930313233343536[-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;][-cat [-ignoreCrc] &lt;src&gt; ...][-checksum &lt;src&gt; ...][-chgrp [-R] GROUP PATH...][-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...][-chown [-R] [OWNER][:[GROUP]] PATH...][-copyFromLocal [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;][-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;][-count [-q] &lt;path&gt; ...][-cp [-f] [-p] &lt;src&gt; ... &lt;dst&gt;][-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]][-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;][-df [-h] [&lt;path&gt; ...]][-du [-s] [-h] &lt;path&gt; ...][-expunge][-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;][-getfacl [-R] &lt;path&gt;][-getmerge [-nl] &lt;src&gt; &lt;localdst&gt;][-help [cmd ...]][-ls [-d] [-h] [-R] [&lt;path&gt; ...]][-mkdir [-p] &lt;path&gt; ...][-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;][-moveToLocal &lt;src&gt; &lt;localdst&gt;][-mv &lt;src&gt; ... &lt;dst&gt;][-put [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;][-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;][-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...][-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...][-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]][-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...][-stat [format] &lt;path&gt; ...][-tail [-f] &lt;file&gt;][-test -[defsz] &lt;path&gt;][-text [-ignoreCrc] &lt;src&gt; ...][-touchz &lt;path&gt; ...][-usage [cmd ...]] 3.常用命令参数介绍 -help 功能：输出这个命令参数手册 -ls 功能：显示目录信息 示例： hadoop fs -ls hdfs://hadoop-server01:9000/ 备注：这些参数中，所有的hdfs**路径都可以简写 –&gt;hadoop fs -ls / 等同于上一条命令的效果 -mkdir 功能：在hdfs**上创建目录* 示例：hadoop fs -mkdir -p /aaa/bbb/cc/dd* -moveFromLocal 功能：从本地剪切粘贴到hdfs 示例：hadoop fs - moveFromLocal /home/hadoop/a.txt /aaa/bbb/cc/dd -moveToLocal 功能：从hdfs**剪切粘贴到本地* 示例：hadoop fs - moveToLocal /aaa/bbb/cc/dd /home/hadoop/a.txt* –appendToFile 功能：追加一个文件到已经存在的文件末尾 示例：hadoop fs -appendToFile ./hello.txt hdfs://hadoop-server01:9000/hello.txt 可以简写为： Hadoop fs -appendToFile ./hello.txt /hello.txt -cat 功能：显示文件内容 示例：hadoop fs -cat /hello.txt -tail 功能：显示一个文件的末尾 示例：hadoop fs -tail /weblog/access_log.1 -text 功能：以字符形式打印一个文件的内容 示例：hadoop fs -text /weblog/access_log.1 -chgrp -chmod -chown 功能：linux**文件系统中的用法一样，对文件所属权限* 示例： hadoop fs -chmod 666 /hello.txt hadoop fs -chown someuser:somegrp /hello.txt* -copyFromLocal 功能：从本地文件系统中拷贝文件到hdfs**路径去 示例：hadoop fs -copyFromLocal ./jdk.tar.gz /aaa/ -copyToLocal 功能：从hdfs**拷贝到本地 示例：hadoop fs -copyToLocal /aaa/jdk.tar.gz -cp 功能：从hdfs**的一个路径拷贝hdfs**的另一个路径 示例： hadoop fs -cp /aaa/jdk.tar.gz /bbb/jdk.tar.gz.2 -mv 功能：在hdfs**目录中移动文件* 示例： hadoop fs -mv /aaa/jdk.tar.gz /* -get 功能：等同于copyToLocal**，就是从hdfs**下载文件到本地 示例：hadoop fs -get /aaa/jdk.tar.gz - 功能：合并下载多个文件 示例：比getmerge *如hdfs的目录 /aaa/*下有多个文件:log.1, log.2,log.3,… hadoop fs -getmerge /aaa/log.* ./log.sum -put 功能：等同于copyFromLocal 示例：hadoop fs -put /aaa/jdk.tar.gz /bbb/jdk.tar.gz.2 -rm 功能：删除文件或文件夹 示例：hadoop fs -rm -r /aaa/bbb/ -rmdir 功能：删除空目录 示例：hadoop fs -rmdir /aaa/bbb/ccc -df 功能：统计文件系统的可用空间信息 示例：hadoop fs -df -h / -du 功能：统计文件夹的大小信息 示例： *hadoop fs -du -s -h /aaa/** -count 功能：统计一个指定目录下的文件节点数量 示例：hadoop fs -count /aaa/ -setrep 功能：设置hdfs**中文件的副本数量* 示例：hadoop fs -setrep 3 /aaa/jdk.tar.gz* 查看dfs集群工作状态的命令 hdfs dfsadmin -report]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3.hdfs-javaAPI]]></title>
    <url>%2F2018%2F09%2F20%2F3.hdfs-javaAPI%2F</url>
    <content type="text"><![CDATA[[TOC] 1.java操作hdfs环境搭建1.1创建mvn项目添加依赖12345678910111213141516171819202122232425&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.6.4&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.6.4&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.6.4&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;jdk.tools&lt;/groupId&gt; &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt; &lt;version&gt;1.6&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;$&#123;JAVA_HOME&#125;/lib/tools.jar&lt;/systemPath&gt;&lt;/dependency&gt; 1.2测试1234567891011// 获取/路径下面的所有文件Configuration configuration = new Configuration();configuration.set("fs.defaultFS", "hdfs://min1:9000");FileSystem fs = FileSystem.get(configuration);RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path("/"), true);while(listFiles.hasNext()) &#123; LocatedFileStatus fileStatus = listFiles.next(); String name = fileStatus.getPath().getName(); //打印文件名 System.out.println(name);&#125;fs.close(); 1.3结果每个人的结果不同。本人的下面有三个文件： 123linux.logwindows.logprofile 2.hdfs客户端权限身份伪造的问题为什么需要权限身份伪造？ 123// 执行下面代码，需求是把本地文件上传到hdfs的/路径下fs.copyFromLocalFile(new Path(&quot;d://a.txt&quot;), new Path(&quot;/&quot;));fs.close(); 不幸的是报错了，说Administrator这个用户没有/的权限，/路径的用户是root组是supergroup，所有我们现在需要去伪造用户成root. 12org.apache.hadoop.security.AccessControlException: Permission denied: user=Administrator, access=WRITE, inode=&quot;/&quot;:root:supergroup:drwxr-xr-x at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkFsPermission(FSPermissionChecker.java:271) 处理方式： 123456789101112131415方式一： Configuration configuration = new Configuration(); configuration.set(&quot;fs.defaultFS&quot;, &quot;hdfs://min1:9000&quot;); try &#123; System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;root&quot;); fs = FileSystem.get(configuration); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; 注意：System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;root&quot;);只能放在fs = FileSystem.get(configuration);前面，不然你就彩蛋了。方式二： Configuration configuration = new Configuration(); fs = FileSystem.get(new URI(&quot;hdfs://min1:9000&quot;), configuration, &quot;root&quot;);方式三：百度一下 3.windows平台下开发hadoop需要注意的细节运行下面代码把hdfs中文件拷贝到windows下。下载功能 12fs.copyToLocalFile(new Path(&quot;/a.txt&quot;), new Path(&quot;d:/&quot;));fs.close(); 我去又报错 123456789101112131415161718192021Exception in thread &quot;main&quot; java.lang.NullPointerException at java.lang.ProcessBuilder.start(ProcessBuilder.java:1012) at org.apache.hadoop.util.Shell.runCommand(Shell.java:482) at org.apache.hadoop.util.Shell.run(Shell.java:455) at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715) at org.apache.hadoop.util.Shell.execCommand(Shell.java:808) at org.apache.hadoop.util.Shell.execCommand(Shell.java:791) at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:659) at org.apache.hadoop.fs.FilterFileSystem.setPermission(FilterFileSystem.java:490) at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:462) at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:428) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:786) at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:365) at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:338) at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289) at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:1970) at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:1939) at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:1915) at cn.mytest.hdfs.TestHDFS.main(TestHDFS.java:30) 原因： 1hadoop要与windows打交道，需要通过和windows的相关的类库进行交互操作文件。而本地不存在hadoop交互的类库。 建议： 123456789方式一：建议在linux下进行hadoop应用的开发，不会存在兼容性问题。如在window上做客户端应用开发，需要设置以下环境：A、用给的windows平台下编译的hadoop安装包解压一份到windows的任意一个目录下B、在window系统中配置HADOOP_HOME指向你解压的安装包目录C、在windows系统的path变量中加入HADOOP_HOME的bin目录方式二:采用java自己的io直接写入文件fs.copyToLocalFile(false, new Path(&quot;/a.txt&quot;), new Path(&quot;d:/&quot;), true); 4.hdfs客户端文件上传12345678910111213141516171819202122232425262728293031323334353637FileSystem fs = null; @Before public void init() throws Exception &#123; // 构造一个配置参数对象，设置一个参数：我们要访问的hdfs的URI // 从而FileSystem.get()方法就知道应该是去构造一个访问hdfs文件系统的客户端，以及hdfs的访问地址 // new Configuration();的时候，它就会去加载jar包中的hdfs-default.xml // 然后再加载classpath下的hdfs-site.xml Configuration conf = new Configuration(); // conf.set("fs.defaultFS", "hdfs://mini1:9000"); //参数优先级： 1、客户端代码中设置的值 2、classpath下的用户自定义配置文件 3、然后是服务器的默认配置 /*conf.set("dfs.replication", "2"); conf.set("dfs.block.size", "64m");*/ // 获取一个hdfs的访问客户端，根据参数，这个实例应该是DistributedFileSystem的实例 // fs = FileSystem.get(conf); // 如果这样去获取，那conf里面就可以不要配"fs.defaultFS"参数，而且，这个客户端的身份标识已经是root用户 fs = FileSystem.get(new URI("hdfs://mini1:9000"), conf, "root"); &#125;方式一： public void testAddFileToHdfs() throws Exception &#123; Path src = new Path("d://a.txt"); Path dst = new Path("/"); fs.copyFromLocalFile(src, dst); fs.close(); &#125;方式二流: FileInputStream in = new FileInputStream("d://a.txt"); Path path = new Path("/aa.txt"); FSDataOutputStream out = fs.create(path); IOUtils.copy(in, out); fs.close(); 5.hdfs客户端文件下载12345678方式一: fs.copyToLocalFile(false, new Path("/a.txt"), new Path("d:/"), true); fs.close();方式二流： FSDataInputStream in = fs.open(new Path("/a.txt")); FileOutputStream out = new FileOutputStream(new File("d:/a.txt")); IOUtils.copy(in, out); fs.close(); 6.hdfs客户端目录操作、查看文件夹以及文件信息123456789101112131415161718//查看目录信息，只显示文件RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path("/"), true);while (listFiles.hasNext()) &#123; LocatedFileStatus fileStatus = listFiles.next(); System.out.println(fileStatus.getPath().getName()); System.out.println(fileStatus.getBlockSize()); System.out.println(fileStatus.getPermission()); System.out.println(fileStatus.getLen()); BlockLocation[] blockLocations = fileStatus.getBlockLocations(); for (BlockLocation bl : blockLocations) &#123; System.out.println("block-length:" + bl.getLength() + "--" + "block-offset:" + bl.getOffset()); String[] hosts = bl.getHosts(); for (String host : hosts) &#123; System.out.println(host); &#125; &#125; System.out.println("--------------分割线--------------");&#125; 123456789101112// 查看文件及文件夹信息FileStatus[] listStatus = fs.listStatus(new Path("/"));String flag = "";for (FileStatus fstatus : listStatus) &#123; if (fstatus.isFile()) &#123; flag = "文件"; &#125; else &#123; flag = "文件夹"; &#125; System.out.println(flag + fstatus.getPath().getName()); System.out.println(fstatus.getPermission());&#125; 1234567891011// hdfs支持随机定位进行文件读取，而且可以方便地读取指定长度 用于上层分布式运算框架并发处理数据// 先获取一个文件的输入流----针对hdfs上的FSDataInputStream in = fs.open(new Path("/a.txt"));// 可以将流的起始偏移量进行自定义in.seek(22);// 再构造一个文件的输出流----针对本地的FileOutputStream out = new FileOutputStream(new File("d:/a.txt"));org.apache.hadoop.io.IOUtils.copyBytes(in, out, 19L, true);]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hdfs</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1.centos7上安装hadoop2.X集群]]></title>
    <url>%2F2018%2F09%2F19%2F1.centos7%E4%B8%8A%E5%AE%89%E8%A3%85hadoop2.X%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[​1.0先将虚拟机的网络模式选为NAT 1.1修改主机名 vi /etc/hostname 1.2修改IP vi /etc/sysconfig/network-scripts/ifcfg-ens33 TYPE=Ethernet PROXY_METHOD=none BROWSER_ONLY=no BOOTPROTO=static ### DEFROUTE=yes IPV4_FAILURE_FATAL=no IPV6INIT=yes IPV6_AUTOCONF=yes IPV6_DEFROUTE=yes IPV6_FAILURE_FATAL=no IPV6_ADDR_GEN_MODE=stable-privacy NAME=ens33 UUID=43e8ca15-c8b3-41ae-b99c-e9587c763cad DEVICE=ens33 GATEWAY=192.168.100.2 ### NETMASK=255.255.255.0 ### IPADDR=192.168.100.10 ### DNS1=114.114.114.114 ### DNS2=8.8.8.8 ### DNS3=192.168.100.1 ### ONBOOT=yes ### ​​ 1.3修改主机名和IP的映射关系​ vim /etc/hosts​​ 192.168.100.10 min1​ 192.168.100.11 min2​ 192.168.100.12 min3​ 192.168.100.13 min4​​ 1.4关闭防火墙​ systemctl stop firewalld​ systemctl disable firewalld​ 1.5重启Linux​ reboot 2.安装JDK 2.1上传alt+p 后出现sftp窗口，然后put d:\xxx\yy\ll\jdk-8u181-linux-x64.tar.gz 2.2解压jdk #解压 tar -zxvf jdk-8u181-linux-x64.tar.gz 2.3将java添加到环境变量中 vim /etc/profile #在文件最后添加 export JAVA_HOME=/opt/jdk1.8 export PATH=$PATH:$JAVA_HOME/bin #刷新配置 source /etc/profile 3.安装hadoop2.6.4​ 先上传hadoop的安装包到服务器上去/opt​ 注意：hadoop2.x的配置文件$HADOOP_HOME/etc/hadoop​ 伪分布式需要修改5个配置文件3.1配置hadoop 第一个：hadoop-env.sh vim hadoop-env.sh #第25行 export JAVA_HOME=/opt/jdk1.8 第二个：core-site.xml &lt;!-- 指定HADOOP所使用的文件系统schema（URI），HDFS的老大（NameNode）的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://min1:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/hadoop-2.6.4/tmp&lt;/value&gt; &lt;/property&gt; 第三个：hdfs-site.xml &lt;!-- 指定HDFS副本的数量 --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.secondary.http.address&lt;/name&gt; &lt;value&gt;192.168.1.152:50090&lt;/value&gt; &lt;/property&gt; &lt;!--自己选择添加----&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data/name,/path2/,/path3/,nfs://&lt;/value&gt; &lt;/property&gt; &lt;!---namenode配置多个目录和datanode配置多个目录，有什么区别？----&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data/data,/path2/&lt;/value&gt; &lt;/property&gt; 第四个：mapred-site.xml (mv mapred-site.xml.template mapred-site.xml) mv mapred-site.xml.template mapred-site.xml vim mapred-site.xml &lt;!-- 指定mr运行在yarn上 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; 第五个：yarn-site.xml &lt;!-- 指定YARN的老大（ResourceManager）的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;min1&lt;/value&gt; &lt;/property&gt; &lt;!-- reducer获取数据的方式 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; 3.2将hadoop添加到环境变量 vim /etc/proflie export JAVA_HOME=/opt/jdk1.8 export HADOOP_HOME=/opt/hadoop-2.6.4 export PATH=${PATH}:${JAVA_HOME}/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin source /etc/profile 3.3格式化namenode（是对namenode进行初始化） hdfs namenode -format (hadoop namenode -format) 3.4启动hadoop 先启动HDFS sbin/start-dfs.sh 再启动YARN sbin/start-yarn.sh 3.5验证是否启动成功 使用jps命令验证 27408 NameNode 28218 Jps 27643 SecondaryNameNode 28066 NodeManager 27803 ResourceManager 27512 DataNode http://192.168.100.10:50070 （HDFS管理界面） http://192.168.100.10:8088 （MR管理界面） 4.配置ssh免登陆​ #生成ssh免登陆密钥 #进入到我的home目录 cd ~/.ssh ssh-keygen -t rsa （四个回车） 执行完这个命令后，会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥） 将公钥拷贝到要免密登陆的目标机器上 ssh-copy-id localhost 启动命令​ 1234567891011121314151617单一的启动方式hadoop-daemon.sh start namenodehadoop-daemon.sh start datanodehadoop-daemon.sh start secondarynamenode批量启动 先要去修改 vi /opt/hadoop-2.6.4/etc/hadoop/slaves 把需要启动的主机添加到文件中 min1 min2 min3start-dfs.sh验证http://192.168.100.10:50070hdfs dfs -put 本地文件路径 hdfs的文件路径hdfs dfs -put /etc/profile /]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[什么是鸡汤？]]></title>
    <url>%2F2018%2F08%2F16%2F%E9%B8%A1%E6%B1%A4%2F</url>
    <content type="text"><![CDATA[学习分什么高低贵贱，从零好好开始。 2018年8月16日10:54:09明确的说今天我们还不足够的优秀，那就多读书。 2018年8月17日09:31:36]]></content>
      <categories>
        <category>鸡汤</category>
      </categories>
      <tags>
        <tag>鸡汤</tag>
      </tags>
  </entry>
</search>
