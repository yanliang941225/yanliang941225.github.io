<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[1.centos7上安装hadoop2.X集群]]></title>
    <url>%2F2018%2F09%2F19%2F1.centos7%E4%B8%8A%E5%AE%89%E8%A3%85hadoop2.X%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[2.1.准备Linux环境​ 1.0先将虚拟机的网络模式选为NAT 1.1修改主机名 vi /etc/hostname 1.2修改IP vi /etc/sysconfig/network-scripts/ifcfg-ens33 TYPE=Ethernet PROXY_METHOD=none BROWSER_ONLY=no BOOTPROTO=static ### DEFROUTE=yes IPV4_FAILURE_FATAL=no IPV6INIT=yes IPV6_AUTOCONF=yes IPV6_DEFROUTE=yes IPV6_FAILURE_FATAL=no IPV6_ADDR_GEN_MODE=stable-privacy NAME=ens33 UUID=43e8ca15-c8b3-41ae-b99c-e9587c763cad DEVICE=ens33 GATEWAY=192.168.100.2 ### NETMASK=255.255.255.0 ### IPADDR=192.168.100.10 ### DNS1=114.114.114.114 ### DNS2=8.8.8.8 ### DNS3=192.168.100.1 ### ONBOOT=yes ### ​​ 1.3修改主机名和IP的映射关系​ vim /etc/hosts​​ 192.168.100.10 min1​ 192.168.100.11 min2​ 192.168.100.12 min3​ 192.168.100.13 min4​​ 1.4关闭防火墙​ systemctl stop firewalld​ systemctl disable firewalld​ 1.5重启Linux​ reboot 2.安装JDK 2.1上传alt+p 后出现sftp窗口，然后put d:\xxx\yy\ll\jdk-8u181-linux-x64.tar.gz 2.2解压jdk #解压 tar -zxvf jdk-8u181-linux-x64.tar.gz 2.3将java添加到环境变量中 vim /etc/profile #在文件最后添加 export JAVA_HOME=/opt/jdk1.8 export PATH=$PATH:$JAVA_HOME/bin #刷新配置 source /etc/profile 3.安装hadoop2.6.4​ 先上传hadoop的安装包到服务器上去/opt​ 注意：hadoop2.x的配置文件$HADOOP_HOME/etc/hadoop​ 伪分布式需要修改5个配置文件3.1配置hadoop 第一个：hadoop-env.sh vim hadoop-env.sh #第25行 export JAVA_HOME=/opt/jdk1.8 第二个：core-site.xml &lt;!-- 指定HADOOP所使用的文件系统schema（URI），HDFS的老大（NameNode）的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://min1:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/hadoop-2.6.4/tmp&lt;/value&gt; &lt;/property&gt; 第三个：hdfs-site.xml &lt;!-- 指定HDFS副本的数量 --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.secondary.http.address&lt;/name&gt; &lt;value&gt;192.168.1.152:50090&lt;/value&gt; &lt;/property&gt; &lt;!--自己选择添加----&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data/name,/path2/,/path3/,nfs://&lt;/value&gt; &lt;/property&gt; &lt;!---namenode配置多个目录和datanode配置多个目录，有什么区别？----&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data/data,/path2/&lt;/value&gt; &lt;/property&gt; 第四个：mapred-site.xml (mv mapred-site.xml.template mapred-site.xml) mv mapred-site.xml.template mapred-site.xml vim mapred-site.xml &lt;!-- 指定mr运行在yarn上 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; 第五个：yarn-site.xml &lt;!-- 指定YARN的老大（ResourceManager）的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;min1&lt;/value&gt; &lt;/property&gt; &lt;!-- reducer获取数据的方式 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; 3.2将hadoop添加到环境变量 vim /etc/proflie export JAVA_HOME=/opt/jdk1.8 export HADOOP_HOME=/opt/hadoop-2.6.4 export PATH=${PATH}:${JAVA_HOME}/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin source /etc/profile 3.3格式化namenode（是对namenode进行初始化） hdfs namenode -format (hadoop namenode -format) 3.4启动hadoop 先启动HDFS sbin/start-dfs.sh 再启动YARN sbin/start-yarn.sh 3.5验证是否启动成功 使用jps命令验证 27408 NameNode 28218 Jps 27643 SecondaryNameNode 28066 NodeManager 27803 ResourceManager 27512 DataNode http://192.168.100.10:50070 （HDFS管理界面） http://192.168.100.10:8088 （MR管理界面） 4.配置ssh免登陆​ #生成ssh免登陆密钥 #进入到我的home目录 cd ~/.ssh ssh-keygen -t rsa （四个回车） 执行完这个命令后，会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥） 将公钥拷贝到要免密登陆的目标机器上 ssh-copy-id localhost 启动命令​ 1234567891011121314151617单一的启动方式hadoop-daemon.sh start namenodehadoop-daemon.sh start datanodehadoop-daemon.sh start secondarynamenode批量启动 先要去修改 vi /opt/hadoop-2.6.4/etc/hadoop/slaves 把需要启动的主机添加到文件中 min1 min2 min3start-dfs.sh验证http://192.168.100.10:50070hdfs dfs -put 本地文件路径 hdfs的文件路径hdfs dfs -put /etc/profile /]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[什么是鸡汤？]]></title>
    <url>%2F2018%2F08%2F16%2F%E9%B8%A1%E6%B1%A4%2F</url>
    <content type="text"><![CDATA[学习分什么高低贵贱，从零好好开始。 2018年8月16日10:54:09明确的说今天我们还不足够的优秀，那就多读书。 2018年8月17日09:31:36]]></content>
      <categories>
        <category>鸡汤</category>
      </categories>
      <tags>
        <tag>鸡汤</tag>
      </tags>
  </entry>
</search>
