<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.4.0" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.4.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.4.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.4.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.4.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '6.4.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta property="og:type" content="website">
<meta property="og:title" content="代码块工作室">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="代码块工作室">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="代码块工作室">






  <link rel="canonical" href="http://yoursite.com/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>代码块工作室</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">代码块工作室</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home menu-item-active">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br />Home</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
    <a href="/tags/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />Tags</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">
    <a href="/categories/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-th"></i> <br />Categories</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />Archives</a>
  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/28/4.mapreduce程序编写/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="严亮">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="代码块工作室">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/28/4.mapreduce程序编写/" itemprop="url">
                  4.mapreduce程序编写
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-09-28 14:19:08" itemprop="dateCreated datePublished" datetime="2018-09-28T14:19:08+08:00">2018-09-28</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2018-09-26 15:01:55" itemprop="dateModified" datetime="2018-09-26T15:01:55+08:00">2018-09-26</time>
              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/大数据/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="MAPREDUCE-原理"><a href="#MAPREDUCE-原理" class="headerlink" title="MAPREDUCE 原理"></a>MAPREDUCE 原理</h1><p>Mapreduce 是一个分布式运算程序的*<em>编程框架<strong>，</strong>是用户开发“基于hadoop**的数据分析应用”的核心框架；</em></p>
<p>Mapreduce<strong>核心功能</strong>是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个hadoop集群上</p>
<h2 id="为什么要MAPREDUCE"><a href="#为什么要MAPREDUCE" class="headerlink" title="为什么要MAPREDUCE"></a>为什么要MAPREDUCE</h2><p>（1）海量数据在单机上处理因为硬件资源限制，无法胜任<br>（2）而一旦将单机版程序扩展到集群来分布式运行，将极大增加程序的复杂度和开发难度<br>（3）引入mapreduce框架后，开发人员可以将绝大部分工作集中在业务逻辑的开发上，而将分布式计算中的复杂性交由框架来处理</p>
<h2 id="流程解析"><a href="#流程解析" class="headerlink" title="流程解析"></a>流程解析</h2><p>1、    一个mr程序启动的时候，最先启动的是MRAppMaster，MRAppMaster启动后根据本次job的描述信息，计算出需要的maptask实例数量，然后向集群申请机器启动相应数量的maptask进程</p>
<p>2、    maptask进程启动之后，根据给定的数据切片范围进行数据处理，主体流程为：<br>a)    利用客户指定的inputformat来获取RecordReader读取数据，形成输入KV对<br>b)    将输入KV对传递给客户定义的map()方法，做逻辑运算，并将map()方法输出的KV对收集到缓存<br>c)    将缓存中的KV对按照K分区排序后不断溢写到磁盘文件</p>
<p>3、    MRAppMaster监控到所有maptask进程任务完成之后，会根据客户指定的参数启动相应数量的reducetask进程，并告知reducetask进程要处理的数据范围（数据分区）</p>
<p>4、    Reducetask进程启动之后，根据MRAppMaster告知的待处理数据所在位置，从若干台maptask运行所在机器上获取到若干个maptask输出结果文件，并在本地进行重新归并排序，然后按照相同key的KV为一个组，调用客户定义的reduce()方法进行逻辑运算，并收集运算输出的结果KV，然后调用客户指定的outputformat将结果数据输出到外部存储</p>
<h1 id="MAPREDUCE-实践"><a href="#MAPREDUCE-实践" class="headerlink" title="MAPREDUCE 实践"></a>MAPREDUCE 实践</h1><h2 id="MAPREDUCE-示例编写及编程规范"><a href="#MAPREDUCE-示例编写及编程规范" class="headerlink" title="MAPREDUCE 示例编写及编程规范"></a>MAPREDUCE 示例编写及编程规范</h2><p>（1）用户编写的程序分成三个部分：Mapper，Reducer，Driver(提交运行mr程序的客户端)<br>（2）Mapper的输入数据是KV对的形式（KV的类型可自定义）<br>（3）Mapper的输出数据是KV对的形式（KV的类型可自定义）<br>（4）Mapper中的业务逻辑写在map()方法中<br>（5）map()方法（maptask进程）对每一个&lt;K,V&gt;调用一次<br>（6）Reducer的输入数据类型对应Mapper的输出数据类型，也是KV<br>（7）Reducer的业务逻辑写在reduce()方法中<br>（8）Reducetask进程对每一组相同k的&lt;k,v&gt;组调用一次reduce()方法<br>（9）用户自定义的Mapper和Reducer都要继承各自的父类<br>（10）整个程序需要一个Drvier来进行提交，提交的是一个描述了各种必要信息的job对象</p>
<h2 id="单词统计实例"><a href="#单词统计实例" class="headerlink" title="单词统计实例"></a>单词统计实例</h2><h3 id="定义一个mapper类"><a href="#定义一个mapper类" class="headerlink" title="定义一个mapper类"></a>定义一个mapper类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.codingkuai.mapreduce;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Mapper&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt; 介紹</span></span><br><span class="line"><span class="comment"> * KEYIN: 是值框架读取到的数据的key类型</span></span><br><span class="line"><span class="comment"> * 		  在默认的读取数据组件InputFormat下，读取的key是一行文本的偏移量，所以key的流行是long类型的</span></span><br><span class="line"><span class="comment"> * VALUEIN：指框架读取到数据的value类型</span></span><br><span class="line"><span class="comment"> * 		在默认的读取数据组件InputFormat下，读到的value就是一行文本的内容，所以value的类型是String的</span></span><br><span class="line"><span class="comment"> * KEYOUT：是指用户自定义逻辑方法返回的数据中的key的类型 这个是由用户业务逻辑决定的。</span></span><br><span class="line"><span class="comment"> * 		在我们的单词统计当中，我们输出的是单词作为key，所以类型是String</span></span><br><span class="line"><span class="comment"> * VALUEOUT：是指用户自定义逻辑方法返回的数据中value的类型 这个是由用户业务逻辑决定的。</span></span><br><span class="line"><span class="comment"> * 		在我们的单词统计当中，我们输出的是单词数量作为value，所以类型是Integer</span></span><br><span class="line"><span class="comment"> * 但是，String, Long都是jdk中自带的数据类型，在序列化的时候，效率比较低，hadoop为了提高序列化效率，他就 自定义了一套数据类型。</span></span><br><span class="line"><span class="comment"> * Long  -&gt;  LongWritable</span></span><br><span class="line"><span class="comment"> * String -&gt; Text</span></span><br><span class="line"><span class="comment"> * Integer -&gt; IntWritable</span></span><br><span class="line"><span class="comment"> * null -&gt; NullWritable</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> yanliang</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt;</span>&#123;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * 这个map方法就是mapreduce程序中被主题程序MapTask所调用的用户业务逻辑方法</span></span><br><span class="line"><span class="comment">	 * MapTask会驱动我们的读取数据组件InputFormat去读取数据(KEYIN，VALUE),每读取一个（K， V）,他就会传入到这个用户写的map方法中去调用一次</span></span><br><span class="line"><span class="comment">	 * 在默认的inputFormat冲突中，此处的key就是一行的起始偏移量，value就是一行的内容</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, IntWritable&gt;.Context context)</span></span></span><br><span class="line"><span class="function">			<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		String line = value.toString();</span><br><span class="line">		String[] split = line.split(<span class="string">" "</span>);</span><br><span class="line">		<span class="keyword">for</span> (String word : split) &#123;</span><br><span class="line">			context.write(<span class="keyword">new</span> Text(word), <span class="keyword">new</span> IntWritable(<span class="number">1</span>));</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="定义一个reducer类"><a href="#定义一个reducer类" class="headerlink" title="定义一个reducer类"></a>定义一个reducer类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.codingkuai.mapreduce;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * reducetask在调用我们的reduce方法</span></span><br><span class="line"><span class="comment"> * reducetask应该接受到map阶段中所有maptask输出的数据中的一部分</span></span><br><span class="line"><span class="comment"> * (key.hashcode % numReduceTask == 本ReduceTask编号)</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * reducetask将接受到的kv数据拿来处理时，是这样调用我们的reduce方法的：</span></span><br><span class="line"><span class="comment"> * 先将自己接受到所有kv对按照K分组（根据K是否相同）</span></span><br><span class="line"><span class="comment"> * 然后将一组kv中的K传给我们的reduce方法的key变量，把这一组kv中所有v用一个迭代器传给reduce方法的变量values</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> yanliang</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountReduce</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values,</span></span></span><br><span class="line"><span class="function"><span class="params">			Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		<span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">		 </span><br><span class="line">		<span class="keyword">for</span> (IntWritable v : values) &#123;</span><br><span class="line">			<span class="keyword">int</span> i = v.get();</span><br><span class="line">			count += i;</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		context.write(key, <span class="keyword">new</span> IntWritable(count));</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="定义一个主类，用来描述job并提交job"><a href="#定义一个主类，用来描述job并提交job" class="headerlink" title="定义一个主类，用来描述job并提交job"></a>定义一个主类，用来描述job并提交job</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.codingkuai.mapreduce;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 本类是客户端用来指定wordCount job程序运行时候所需要的很多参数 比如：指定哪一个类左右map阶段的业务逻辑类</span></span><br><span class="line"><span class="comment">  * 哪个类作为reduce阶段的业务逻辑类 指定用那个组件作为数据的读取组件 数据结果输出组件 指定这个wordcount jar包锁在的路径</span></span><br><span class="line"><span class="comment"> * 以及其它各种所需要的参数</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> yanliang</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountDriver</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">		Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">		System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"root"</span>);</span><br><span class="line">		conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://min1:9000"</span>);</span><br><span class="line"><span class="comment">//		conf.set("mapreduce.framework.name", "yarn");</span></span><br><span class="line"><span class="comment">//		conf.set("yarn.resourcemanager.hostname", "min1");</span></span><br><span class="line">		Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 告诉框架，我们的程序所有jar包的位置.linux上的路径</span></span><br><span class="line"><span class="comment">//		job.setJar("/opt/temp/wordcount.jar");</span></span><br><span class="line">		job.setJarByClass(WordCountDriver.class);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 告诉框架，我们程序所用的mapper和reduce类是什么</span></span><br><span class="line">		job.setMapperClass(WordCountMapper.class);</span><br><span class="line">		job.setReducerClass(WordCountReduce.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 告诉框架，我们程序输出的数据类型</span></span><br><span class="line">		job.setMapOutputKeyClass(Text.class); <span class="comment">// map阶段</span></span><br><span class="line">		job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line">		job.setOutputKeyClass(Text.class); <span class="comment">// 最终结果</span></span><br><span class="line">		job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 告诉框架，我们程序使用的数据读取组件 结果输出所用的组件</span></span><br><span class="line">		<span class="comment">// TextInputFormat是mapreduce程序中内置的一种读取数据组件 准确的说叫做读取文件的输入组件</span></span><br><span class="line">		job.setInputFormatClass(TextInputFormat.class);</span><br><span class="line">		job.setOutputFormatClass(TextOutputFormat.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 告诉框架，我们要处理的数据文件在哪个路径下</span></span><br><span class="line">		FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"/wordcount/input"</span>));</span><br><span class="line">		FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"/wordcount/output"</span>));</span><br><span class="line"></span><br><span class="line">		<span class="keyword">boolean</span> res = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">		System.exit(res ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">		<span class="comment">// hadoop 上运行</span></span><br><span class="line">		<span class="comment">//hadoop jar wordcount.jar cn.codingkuai.mapreduce.WordCountDriver</span></span><br><span class="line">        <span class="comment">// 插件结果</span></span><br><span class="line">		<span class="comment">//hadoop fs -cat /wordcount/output/part-r-00000</span></span><br><span class="line">		<span class="comment">/*</span></span><br><span class="line"><span class="comment">		 *  guo	1</span></span><br><span class="line"><span class="comment">			hello	7</span></span><br><span class="line"><span class="comment">			nihao	2</span></span><br><span class="line"><span class="comment">			Re.	1</span></span><br><span class="line"><span class="comment">			shi	1</span></span><br><span class="line"><span class="comment">			tom	1</span></span><br><span class="line"><span class="comment">			tom1	1</span></span><br><span class="line"><span class="comment">			uu	1</span></span><br><span class="line"><span class="comment">			wo	1</span></span><br><span class="line"><span class="comment">			yanliang	1</span></span><br><span class="line"><span class="comment">			zhong	1</span></span><br><span class="line"><span class="comment">			zhongguo	1</span></span><br><span class="line"><span class="comment">		 */</span></span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="MAPREDUCE程序运行模式"><a href="#MAPREDUCE程序运行模式" class="headerlink" title="MAPREDUCE程序运行模式"></a>MAPREDUCE程序运行模式</h2><h3 id="本地运行模式"><a href="#本地运行模式" class="headerlink" title="本地运行模式"></a>本地运行模式</h3><p>（1）mapreduce程序是被提交给LocalJobRunner在本地以单进程的形式运行</p>
<p>（2）而处理的数据及输出结果可以在本地文件系统，也可以在hdfs上</p>
<p>（3）怎样实现本地运行？写一个程序，不要带集群的配置文件（本质是你的mr程序的conf中是否有mapreduce.framework.name=local以及yarn.resourcemanager.hostname参数）</p>
<p><em>（4）<strong>本地模式非常便于进行业务逻辑的debug</strong>，只要在eclipse**中打断点即可</em></p>
<p><em>如果在windows<strong>下想运行本地模式来测试程序逻辑，需要在windows</strong>中配置环境变量：</em></p>
<p><em>％HADOOP_HOME**％  =  d:/hadoop-2.6.1</em></p>
<p><em>%PATH% =</em>  <em>％HADOOP_HOME**％\bin</em></p>
<p><em>并且要将d:/hadoop-2.6.1<strong>的lib</strong>和bin<strong>目录替换成windows</strong>平台编译的版本</em></p>
<h1 id="项目细节"><a href="#项目细节" class="headerlink" title="项目细节"></a>项目细节</h1><h2 id="自定义对象实现MR中的序列化接口"><a href="#自定义对象实现MR中的序列化接口" class="headerlink" title="自定义对象实现MR中的序列化接口"></a>自定义对象实现MR中的序列化接口</h2><p>如果需要将自定义的bean放在key中传输，则还需要实现comparable接口，因为mapreduce框中的shuffle过程一定会对key进行排序,此时，自定义的bean实现的接口应该是：</p>
<p>public  class  FlowBean  implements  WritableComparable<flowbean> </flowbean></p>
<p>需要自己实现的方法是：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.codingkuai;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowBean</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">FlowBean</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">long</span> upFlow;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">long</span> downFlow;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">long</span> sumFlow;</span><br><span class="line">	</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">(<span class="keyword">long</span> upFlow, <span class="keyword">long</span> downFlow)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">super</span>();</span><br><span class="line">		<span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">		<span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">		<span class="keyword">this</span>.sumFlow = upFlow + downFlow;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">(<span class="keyword">long</span> upFlow, <span class="keyword">long</span> downFlow, <span class="keyword">long</span> sumFlow)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">super</span>();</span><br><span class="line">		<span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">		<span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">		<span class="keyword">this</span>.sumFlow = sumFlow;</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">set</span><span class="params">(<span class="keyword">long</span> upFlow, <span class="keyword">long</span> downFlow)</span> </span>&#123;</span><br><span class="line">	    <span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">	    <span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">	    <span class="keyword">this</span>.sumFlow = (upFlow + downFlow);</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getUpFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> upFlow;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setUpFlow</span><span class="params">(<span class="keyword">long</span> upFlow)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getDownFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> downFlow;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setDownFlow</span><span class="params">(<span class="keyword">long</span> downFlow)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getSumFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> sumFlow;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSumFlow</span><span class="params">(<span class="keyword">long</span> sumFlow)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.sumFlow = sumFlow;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * 序列化</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">		out.writeLong(<span class="keyword">this</span>.upFlow);</span><br><span class="line">		out.writeLong(<span class="keyword">this</span>.downFlow);</span><br><span class="line">		out.writeLong(<span class="keyword">this</span>.sumFlow);</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * 反序列化，顺序和序列化顺序一样</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.upFlow = in.readLong();</span><br><span class="line">		<span class="keyword">this</span>.downFlow = in.readLong();</span><br><span class="line">		<span class="keyword">this</span>.sumFlow = in.readLong();</span><br><span class="line">	&#125;</span><br><span class="line">		</span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * TextOutputFormat组件输出结果时候调用的是toString方法</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	    <span class="keyword">return</span> <span class="keyword">this</span>.upFlow + <span class="string">"\t"</span> + <span class="keyword">this</span>.downFlow + <span class="string">"\t"</span> + <span class="keyword">this</span>.sumFlow;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowBean o)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> (<span class="keyword">int</span>) (o.getSumFlow() - <span class="keyword">this</span>.getSumFlow());</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="自定义partitioner"><a href="#自定义partitioner" class="headerlink" title="自定义partitioner"></a>自定义partitioner</h2><p>如果我们通过key不同去到不通文件中的需求，这时候我们需要去设置ReduceTask个数默认是一个，其次去编写分片分规则。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.codingkuai;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProvincePartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">FlowBean</span>&gt;</span>&#123;</span><br><span class="line">	<span class="keyword">static</span> HashMap&lt;String, Integer&gt; provinceMap = <span class="keyword">new</span> HashMap&lt;String, Integer&gt;();</span><br><span class="line"></span><br><span class="line">	<span class="keyword">static</span> &#123;</span><br><span class="line"></span><br><span class="line">		provinceMap.put(<span class="string">"135"</span>, <span class="number">0</span>);</span><br><span class="line">		provinceMap.put(<span class="string">"136"</span>, <span class="number">1</span>);</span><br><span class="line">		provinceMap.put(<span class="string">"137"</span>, <span class="number">2</span>);</span><br><span class="line">		provinceMap.put(<span class="string">"138"</span>, <span class="number">3</span>);</span><br><span class="line">		provinceMap.put(<span class="string">"139"</span>, <span class="number">4</span>);</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text key, FlowBean value, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">		Integer code = provinceMap.get(key.toString().substring(<span class="number">0</span>, <span class="number">3</span>));</span><br><span class="line">		<span class="keyword">return</span> code == <span class="keyword">null</span> ? <span class="number">5</span> : code;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">job.setPartitionerClass(ProvincePartitioner.class);</span><br><span class="line">job.setNumReduceTasks(<span class="number">6</span>);</span><br></pre></td></tr></table></figure>
<h2 id="MAPREDUCE中的Combiner"><a href="#MAPREDUCE中的Combiner" class="headerlink" title="MAPREDUCE中的Combiner"></a>MAPREDUCE中的Combiner</h2><p>（1）combiner是MR程序中Mapper和Reducer之外的一种组件</p>
<p>（2）combiner组件的父类就是Reducer</p>
<p>（3）combiner和reducer的区别在于运行的位置：</p>
<p>Combiner是在每一个maptask所在的节点运行</p>
<p>Reducer是接收全局所有Mapper的输出结果；</p>
<p>(4) combiner的意义就是对每一个maptask的输出进行局部汇总，以减小网络传输量</p>
<p>具体实现步骤：</p>
<p>1、  自定义一个combiner继承Reducer，重写reduce方法</p>
<p>2、  在job中设置：  job.setCombinerClass(CustomCombiner.class)</p>
<p>(5) combiner能够应用的前提是不能影响最终的业务逻辑</p>
<p>而且，combiner的输出kv应该跟reducer的输入kv类型要对应起来</p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/26/5.hive安装/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="严亮">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="代码块工作室">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/26/5.hive安装/" itemprop="url">
                  5.hive安装
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-09-26 14:40:32" itemprop="dateCreated datePublished" datetime="2018-09-26T14:40:32+08:00">2018-09-26</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2018-09-28 14:28:56" itemprop="dateModified" datetime="2018-09-28T14:28:56+08:00">2018-09-28</time>
              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/大数据/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Hive只在一个节点上安装即可</p>
<h1 id="安装mysql"><a href="#安装mysql" class="headerlink" title="安装mysql"></a>安装mysql</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">参考：http://www.cnblogs.com/starof/p/4680083.html</span><br></pre></td></tr></table></figure>
<h1 id="配置hive"><a href="#配置hive" class="headerlink" title="配置hive"></a>配置hive</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">（a）配置HIVE_HOME环境变量  vi conf/hive-env.sh 配置其中的$hadoop_home</span><br><span class="line">	HADOOP_HOME=/opt/hadoop-2.6.4</span><br><span class="line">（b）配置元数据库信息   vi  hive-site.xml </span><br><span class="line">	添加如下内容：</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">&lt;value&gt;jdbc:mysql://127.0.0.1:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;</span><br><span class="line">&lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">&lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">&lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">&lt;value&gt;root&lt;/value&gt;</span><br><span class="line">&lt;description&gt;username to use against metastore database&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">&lt;value&gt;963852&lt;/value&gt;</span><br><span class="line">&lt;description&gt;password to use against metastore database&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line">（C）Jline包版本不一致的问题，需要拷贝hive的lib目录中jline.2.12.jar的jar包替换掉hadoop中的 </span><br><span class="line">	cp jline-2.12.jar /opt/hadoop-2.6.4/share/hadoop/yarn/lib/</span><br><span class="line"></span><br><span class="line">(d) 添加环境变量 vi /etc/profile</span><br><span class="line">    export JAVA_HOME=/opt/jdk1.8</span><br><span class="line">    export HADOOP_HOME=/opt/hadoop-2.6.4</span><br><span class="line">    export HIVE_HOME=/opt/hive</span><br><span class="line">    export 	PATH=$&#123;PATH&#125;:$&#123;JAVA_HOME&#125;/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HIVE_HOME/bin</span><br><span class="line"></span><br><span class="line">. /etc/profile </span><br><span class="line"></span><br><span class="line">(e） mysql驱动添加到lib目录下</span><br><span class="line">(f) 启动</span><br><span class="line">	hive</span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/20/3.hdfs-javaAPI/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="严亮">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="代码块工作室">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/20/3.hdfs-javaAPI/" itemprop="url">
                  3.hdfs-javaAPI
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-09-20 10:18:32 / Modified: 11:56:24" itemprop="dateCreated datePublished" datetime="2018-09-20T10:18:32+08:00">2018-09-20</time>
            

            
              

              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/大数据/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>[TOC]</p>
<h3 id="1-java操作hdfs环境搭建"><a href="#1-java操作hdfs环境搭建" class="headerlink" title="1.java操作hdfs环境搭建"></a>1.java操作hdfs环境搭建</h3><h4 id="1-1创建mvn项目添加依赖"><a href="#1-1创建mvn项目添加依赖" class="headerlink" title="1.1创建mvn项目添加依赖"></a>1.1创建mvn项目添加依赖</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">	&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">	&lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;</span><br><span class="line">	&lt;version&gt;2.6.4&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">	&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">	&lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;</span><br><span class="line">	&lt;version&gt;2.6.4&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">	&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">	&lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</span><br><span class="line">	&lt;version&gt;2.6.4&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">	&lt;groupId&gt;jdk.tools&lt;/groupId&gt;</span><br><span class="line">	&lt;artifactId&gt;jdk.tools&lt;/artifactId&gt;</span><br><span class="line">	&lt;version&gt;1.6&lt;/version&gt;</span><br><span class="line">	&lt;scope&gt;system&lt;/scope&gt;</span><br><span class="line">	&lt;systemPath&gt;$&#123;JAVA_HOME&#125;/lib/tools.jar&lt;/systemPath&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>
<h4 id="1-2测试"><a href="#1-2测试" class="headerlink" title="1.2测试"></a>1.2测试</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取/路径下面的所有文件</span></span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">configuration.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://min1:9000"</span>);</span><br><span class="line">FileSystem fs = FileSystem.get(configuration);</span><br><span class="line">RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(<span class="keyword">new</span> Path(<span class="string">"/"</span>), <span class="keyword">true</span>);</span><br><span class="line"><span class="keyword">while</span>(listFiles.hasNext()) &#123;</span><br><span class="line">	LocatedFileStatus fileStatus = listFiles.next();</span><br><span class="line">	String name = fileStatus.getPath().getName(); <span class="comment">//打印文件名</span></span><br><span class="line">	System.out.println(name);</span><br><span class="line">&#125;</span><br><span class="line">fs.close();</span><br></pre></td></tr></table></figure>
<h4 id="1-3结果"><a href="#1-3结果" class="headerlink" title="1.3结果"></a>1.3结果</h4><p>每个人的结果不同。本人的下面有三个文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">linux.log</span><br><span class="line">windows.log</span><br><span class="line">profile</span><br></pre></td></tr></table></figure>
<h3 id="2-hdfs客户端权限身份伪造的问题"><a href="#2-hdfs客户端权限身份伪造的问题" class="headerlink" title="2.hdfs客户端权限身份伪造的问题"></a>2.hdfs客户端权限身份伪造的问题</h3><p>为什么需要权限身份伪造？</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// 执行下面代码，需求是把本地文件上传到hdfs的/路径下</span><br><span class="line">fs.copyFromLocalFile(new Path(&quot;d://a.txt&quot;), new Path(&quot;/&quot;));</span><br><span class="line">fs.close();</span><br></pre></td></tr></table></figure>
<p>不幸的是报错了，说Administrator这个用户没有/的权限，/路径的用户是root组是supergroup，所有我们现在需要去伪造用户成root.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.security.AccessControlException: Permission denied: user=Administrator, access=WRITE, inode=&quot;/&quot;:root:supergroup:drwxr-xr-x</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkFsPermission(FSPermissionChecker.java:271)</span><br></pre></td></tr></table></figure>
<p>处理方式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">方式一：</span><br><span class="line">   		Configuration configuration = new Configuration();</span><br><span class="line">		configuration.set(&quot;fs.defaultFS&quot;, &quot;hdfs://min1:9000&quot;);</span><br><span class="line">		try &#123;</span><br><span class="line">			System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;root&quot;);</span><br><span class="line">			fs = FileSystem.get(configuration);</span><br><span class="line">		&#125; catch (IOException e) &#123;</span><br><span class="line">			e.printStackTrace();</span><br><span class="line">		&#125;</span><br><span class="line">		注意：System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;root&quot;);只能放在fs = FileSystem.get(configuration);前面，不然你就彩蛋了。</span><br><span class="line">方式二：</span><br><span class="line">	Configuration configuration = new Configuration();</span><br><span class="line">	fs = FileSystem.get(new URI(&quot;hdfs://min1:9000&quot;), configuration, &quot;root&quot;);</span><br><span class="line"></span><br><span class="line">方式三：百度一下</span><br></pre></td></tr></table></figure>
<h3 id="3-windows平台下开发hadoop需要注意的细节"><a href="#3-windows平台下开发hadoop需要注意的细节" class="headerlink" title="3.windows平台下开发hadoop需要注意的细节"></a>3.windows平台下开发hadoop需要注意的细节</h3><p>运行下面代码把hdfs中文件拷贝到windows下。下载功能</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fs.copyToLocalFile(new Path(&quot;/a.txt&quot;), new Path(&quot;d:/&quot;));</span><br><span class="line">fs.close();</span><br></pre></td></tr></table></figure>
<p>我去又报错</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.NullPointerException</span><br><span class="line">	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1012)</span><br><span class="line">	at org.apache.hadoop.util.Shell.runCommand(Shell.java:482)</span><br><span class="line">	at org.apache.hadoop.util.Shell.run(Shell.java:455)</span><br><span class="line">	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)</span><br><span class="line">	at org.apache.hadoop.util.Shell.execCommand(Shell.java:808)</span><br><span class="line">	at org.apache.hadoop.util.Shell.execCommand(Shell.java:791)</span><br><span class="line">	at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:659)</span><br><span class="line">	at org.apache.hadoop.fs.FilterFileSystem.setPermission(FilterFileSystem.java:490)</span><br><span class="line">	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:462)</span><br><span class="line">	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:428)</span><br><span class="line">	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)</span><br><span class="line">	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889)</span><br><span class="line">	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:786)</span><br><span class="line">	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:365)</span><br><span class="line">	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:338)</span><br><span class="line">	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289)</span><br><span class="line">	at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:1970)</span><br><span class="line">	at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:1939)</span><br><span class="line">	at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:1915)</span><br><span class="line">	at cn.mytest.hdfs.TestHDFS.main(TestHDFS.java:30)</span><br></pre></td></tr></table></figure>
<p>原因：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop要与windows打交道，需要通过和windows的相关的类库进行交互操作文件。而本地不存在hadoop交互的类库。</span><br></pre></td></tr></table></figure>
<p>建议：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">方式一：</span><br><span class="line">建议在linux下进行hadoop应用的开发，不会存在兼容性问题。如在window上做客户端应用开发，需要设置以下环境：</span><br><span class="line">A、用给的windows平台下编译的hadoop安装包解压一份到windows的任意一个目录下</span><br><span class="line">B、在window系统中配置HADOOP_HOME指向你解压的安装包目录</span><br><span class="line">C、在windows系统的path变量中加入HADOOP_HOME的bin目录</span><br><span class="line"></span><br><span class="line">方式二:</span><br><span class="line">采用java自己的io直接写入文件</span><br><span class="line">fs.copyToLocalFile(false, new Path(&quot;/a.txt&quot;), new Path(&quot;d:/&quot;), true);</span><br></pre></td></tr></table></figure>
<h3 id="4-hdfs客户端文件上传"><a href="#4-hdfs客户端文件上传" class="headerlink" title="4.hdfs客户端文件上传"></a>4.hdfs客户端文件上传</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">FileSystem fs = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Before</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 构造一个配置参数对象，设置一个参数：我们要访问的hdfs的URI</span></span><br><span class="line">		<span class="comment">// 从而FileSystem.get()方法就知道应该是去构造一个访问hdfs文件系统的客户端，以及hdfs的访问地址</span></span><br><span class="line">		<span class="comment">// new Configuration();的时候，它就会去加载jar包中的hdfs-default.xml</span></span><br><span class="line">		<span class="comment">// 然后再加载classpath下的hdfs-site.xml</span></span><br><span class="line">		Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">		<span class="comment">// conf.set("fs.defaultFS", "hdfs://mini1:9000");</span></span><br><span class="line">		<span class="comment">//参数优先级： 1、客户端代码中设置的值 2、classpath下的用户自定义配置文件 3、然后是服务器的默认配置</span></span><br><span class="line">		<span class="comment">/*conf.set("dfs.replication", "2");</span></span><br><span class="line"><span class="comment">		conf.set("dfs.block.size", "64m");*/</span></span><br><span class="line"></span><br><span class="line">		<span class="comment">// 获取一个hdfs的访问客户端，根据参数，这个实例应该是DistributedFileSystem的实例</span></span><br><span class="line">		<span class="comment">// fs = FileSystem.get(conf);</span></span><br><span class="line"></span><br><span class="line">		<span class="comment">// 如果这样去获取，那conf里面就可以不要配"fs.defaultFS"参数，而且，这个客户端的身份标识已经是root用户</span></span><br><span class="line">		fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://mini1:9000"</span>), conf, <span class="string">"root"</span>);</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">方式一：</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testAddFileToHdfs</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">		Path src = <span class="keyword">new</span> Path(<span class="string">"d://a.txt"</span>);</span><br><span class="line">		Path dst = <span class="keyword">new</span> Path(<span class="string">"/"</span>);</span><br><span class="line">		fs.copyFromLocalFile(src, dst);</span><br><span class="line"></span><br><span class="line">		fs.close();</span><br><span class="line">	&#125;</span><br><span class="line">方式二流:</span><br><span class="line">	FileInputStream in = <span class="keyword">new</span> FileInputStream(<span class="string">"d://a.txt"</span>);</span><br><span class="line">	Path path = <span class="keyword">new</span> Path(<span class="string">"/aa.txt"</span>);</span><br><span class="line">	FSDataOutputStream out = fs.create(path);</span><br><span class="line">	IOUtils.copy(in, out);</span><br><span class="line">	fs.close();</span><br></pre></td></tr></table></figure>
<h3 id="5-hdfs客户端文件下载"><a href="#5-hdfs客户端文件下载" class="headerlink" title="5.hdfs客户端文件下载"></a>5.hdfs客户端文件下载</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">方式一:</span><br><span class="line">    fs.copyToLocalFile(<span class="keyword">false</span>, <span class="keyword">new</span> Path(<span class="string">"/a.txt"</span>), <span class="keyword">new</span> Path(<span class="string">"d:/"</span>), <span class="keyword">true</span>);</span><br><span class="line">    fs.close();</span><br><span class="line">方式二流：</span><br><span class="line">    FSDataInputStream in = fs.open(<span class="keyword">new</span> Path(<span class="string">"/a.txt"</span>));</span><br><span class="line">    FileOutputStream out = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(<span class="string">"d:/a.txt"</span>));</span><br><span class="line">    IOUtils.copy(in, out);</span><br><span class="line">    fs.close();</span><br></pre></td></tr></table></figure>
<h3 id="6-hdfs客户端目录操作、查看文件夹以及文件信息"><a href="#6-hdfs客户端目录操作、查看文件夹以及文件信息" class="headerlink" title="6.hdfs客户端目录操作、查看文件夹以及文件信息"></a>6.hdfs客户端目录操作、查看文件夹以及文件信息</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//查看目录信息，只显示文件</span></span><br><span class="line">RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(<span class="keyword">new</span> Path(<span class="string">"/"</span>), <span class="keyword">true</span>);</span><br><span class="line"><span class="keyword">while</span> (listFiles.hasNext()) &#123;</span><br><span class="line">    LocatedFileStatus fileStatus = listFiles.next();</span><br><span class="line">    System.out.println(fileStatus.getPath().getName());</span><br><span class="line">    System.out.println(fileStatus.getBlockSize());</span><br><span class="line">    System.out.println(fileStatus.getPermission());</span><br><span class="line">    System.out.println(fileStatus.getLen());</span><br><span class="line">    BlockLocation[] blockLocations = fileStatus.getBlockLocations();</span><br><span class="line">    <span class="keyword">for</span> (BlockLocation bl : blockLocations) &#123;</span><br><span class="line">        System.out.println(<span class="string">"block-length:"</span> + bl.getLength() + <span class="string">"--"</span> + <span class="string">"block-offset:"</span> + bl.getOffset());</span><br><span class="line">        String[] hosts = bl.getHosts();</span><br><span class="line">        <span class="keyword">for</span> (String host : hosts) &#123;</span><br><span class="line">            System.out.println(host);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    System.out.println(<span class="string">"--------------分割线--------------"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 查看文件及文件夹信息</span></span><br><span class="line">FileStatus[] listStatus = fs.listStatus(<span class="keyword">new</span> Path(<span class="string">"/"</span>));</span><br><span class="line">String flag = <span class="string">""</span>;</span><br><span class="line"><span class="keyword">for</span> (FileStatus fstatus : listStatus) &#123;</span><br><span class="line">    <span class="keyword">if</span> (fstatus.isFile()) &#123;</span><br><span class="line">        flag = <span class="string">"文件"</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        flag = <span class="string">"文件夹"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    System.out.println(flag + fstatus.getPath().getName());</span><br><span class="line">    System.out.println(fstatus.getPermission());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// hdfs支持随机定位进行文件读取，而且可以方便地读取指定长度 用于上层分布式运算框架并发处理数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 先获取一个文件的输入流----针对hdfs上的</span></span><br><span class="line">FSDataInputStream in = fs.open(<span class="keyword">new</span> Path(<span class="string">"/a.txt"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 可以将流的起始偏移量进行自定义</span></span><br><span class="line">in.seek(<span class="number">22</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 再构造一个文件的输出流----针对本地的</span></span><br><span class="line">FileOutputStream out = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(<span class="string">"d:/a.txt"</span>));</span><br><span class="line">org.apache.hadoop.io.IOUtils.copyBytes(in, out, <span class="number">19L</span>, <span class="keyword">true</span>);</span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/20/2.hdfs-shell操作/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="严亮">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="代码块工作室">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/20/2.hdfs-shell操作/" itemprop="url">
                  2.hdfs-shell操作.md
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-09-20 10:18:32 / Modified: 11:56:31" itemprop="dateCreated datePublished" datetime="2018-09-20T10:18:32+08:00">2018-09-20</time>
            

            
              

              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/大数据/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>[TOC]</p>
<h3 id="1-hdfs命令行客户端"><a href="#1-hdfs命令行客户端" class="headerlink" title="1.hdfs命令行客户端"></a>1.hdfs命令行客户端</h3><p>HDFS提供shell命令行客户端，使用方法如下：</p>
<p>可以使用一下两种形式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs   -…  &lt;args&gt;</span><br><span class="line">hdfs dfs    -…  &lt;args&gt;</span><br></pre></td></tr></table></figure>
<h3 id="2-命令行客户端支持的命令参数"><a href="#2-命令行客户端支持的命令参数" class="headerlink" title="2.命令行客户端支持的命令参数"></a>2.命令行客户端支持的命令参数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">[-cat [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">[-checksum &lt;src&gt; ...]</span><br><span class="line">[-chgrp [-R] GROUP PATH...]</span><br><span class="line">[-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</span><br><span class="line">[-chown [-R] [OWNER][:[GROUP]] PATH...]</span><br><span class="line">[-copyFromLocal [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">[-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">[-count [-q] &lt;path&gt; ...]</span><br><span class="line">[-cp [-f] [-p] &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">[-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]]</span><br><span class="line">[-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;]</span><br><span class="line">[-df [-h] [&lt;path&gt; ...]]</span><br><span class="line">[-du [-s] [-h] &lt;path&gt; ...]</span><br><span class="line">[-expunge]</span><br><span class="line">[-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">[-getfacl [-R] &lt;path&gt;]</span><br><span class="line">[-getmerge [-nl] &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">[-help [cmd ...]]</span><br><span class="line">[-ls [-d] [-h] [-R] [&lt;path&gt; ...]]</span><br><span class="line">[-mkdir [-p] &lt;path&gt; ...]</span><br><span class="line">[-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">[-moveToLocal &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">[-mv &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">[-put [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">[-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;]</span><br><span class="line">[-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...]</span><br><span class="line">[-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...]</span><br><span class="line">[-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]]</span><br><span class="line">[-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...]</span><br><span class="line">[-stat [format] &lt;path&gt; ...]</span><br><span class="line">[-tail [-f] &lt;file&gt;]</span><br><span class="line">[-test -[defsz] &lt;path&gt;]</span><br><span class="line">[-text [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">[-touchz &lt;path&gt; ...]</span><br><span class="line">[-usage [cmd ...]]</span><br></pre></td></tr></table></figure>
<h3 id="3-常用命令参数介绍"><a href="#3-常用命令参数介绍" class="headerlink" title="3.常用命令参数介绍"></a>3.常用命令参数介绍</h3><table>
<thead>
<tr>
<th>-help                功能：输出这个命令参数手册</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>-ls</strong>                     <strong>功能：显示目录信息</strong>   <em>示例： hadoop fs -ls hdfs://hadoop-server01:9000/</em>   <em>备注：这些参数中，所有的hdfs**路径都可以简写</em>   <em>–&gt;hadoop fs -ls /</em>     <em>等同于上一条命令的效果</em></td>
</tr>
<tr>
<td><strong>-mkdir</strong>                 <strong>功能：在hdfs**</strong>上创建目录*<em>   </em>示例：hadoop fs    -mkdir  -p    /aaa/bbb/cc/dd*</td>
</tr>
<tr>
<td><strong>-moveFromLocal</strong>               <strong>功能：从本地剪切粘贴到hdfs</strong>   <em>示例：hadoop  fs  -   moveFromLocal  /home/hadoop/a.txt  /aaa/bbb/cc/dd</em>   <strong>-moveToLocal</strong>                 <strong>功能：从hdfs**</strong>剪切粘贴到本地*<em>   </em>示例：hadoop  fs  -   moveToLocal   /aaa/bbb/cc/dd  /home/hadoop/a.txt*</td>
</tr>
<tr>
<td><strong>–appendToFile</strong>     <strong>功能：追加一个文件到已经存在的文件末尾</strong>   <em>示例：hadoop  fs  -appendToFile  ./hello.txt    hdfs://hadoop-server01:9000/hello.txt</em>   <em>可以简写为：</em>   <em>Hadoop  fs  -appendToFile  ./hello.txt    /hello.txt</em></td>
</tr>
<tr>
<td><strong>-cat</strong>     <strong>功能：显示文件内容</strong>       <em>示例：hadoop fs -cat    /hello.txt</em>       <strong>-tail</strong>                    <strong>功能：显示一个文件的末尾</strong>   <em>示例：hadoop  fs    -tail  /weblog/access_log.1</em>   <strong>-text</strong>                     <strong>功能：以字符形式打印一个文件的内容</strong>   <em>示例：hadoop  fs    -text  /weblog/access_log.1</em></td>
</tr>
<tr>
<td><strong>-chgrp</strong>    <strong>-chmod</strong>   <strong>-chown</strong>   <strong>功能：linux**</strong>文件系统中的用法一样，对文件所属权限*<em>   </em>示例：<em>   </em>hadoop  fs  -chmod    666  /hello.txt<em>   </em>hadoop  fs  -chown    someuser:somegrp   /hello.txt*</td>
</tr>
<tr>
<td><strong>-copyFromLocal</strong>       <strong>功能：从本地文件系统中拷贝文件到hdfs**</strong>路径去<strong>   <em>示例：hadoop  fs  -copyFromLocal  ./jdk.tar.gz  /aaa/</em>   </strong>-copyToLocal<strong>         </strong>功能：从hdfs<strong>**拷贝到本地</strong>   <em>示例：hadoop fs -copyToLocal   /aaa/jdk.tar.gz</em></td>
</tr>
<tr>
<td><strong>-cp</strong>                 <strong>功能：从hdfs**</strong>的一个路径拷贝hdfs<strong>**的另一个路径</strong>   <em>示例： hadoop  fs  -cp    /aaa/jdk.tar.gz    /bbb/jdk.tar.gz.2</em>       <strong>-mv</strong>                        <strong>功能：在hdfs**</strong>目录中移动文件*<em>   </em>示例： hadoop  fs  -mv  /aaa/jdk.tar.gz  /*</td>
</tr>
<tr>
<td><strong>-get</strong>                 <strong>功能：等同于copyToLocal**</strong>，就是从hdfs<strong>**下载文件到本地</strong>   示例：hadoop fs -get  /aaa/jdk.tar.gz   <strong>-</strong>            <strong>功能：合并下载多个文件</strong>   <em>示例：<strong>比</strong></em>getmerge<strong>    *如hdfs</strong>的目录 /aaa/*<em>下有多个文件:log.1, log.2,log.3,…</em>   hadoop   fs -getmerge /aaa/log.* ./log.sum</td>
</tr>
<tr>
<td><strong>-put</strong>                   <strong>功能：等同于copyFromLocal</strong>   <em>示例：hadoop  fs  -put  /aaa/jdk.tar.gz  /bbb/jdk.tar.gz.2</em></td>
</tr>
<tr>
<td><strong>-rm</strong>                   <strong>功能：删除文件或文件夹</strong>   <em>示例：hadoop fs -rm   -r /aaa/bbb/</em>       <strong>-rmdir</strong>                    <strong>功能：删除空目录</strong>   <em>示例：hadoop  fs    -rmdir   /aaa/bbb/ccc</em></td>
</tr>
<tr>
<td><strong>-df</strong>                  <strong>功能：统计文件系统的可用空间信息</strong>   <em>示例：hadoop  fs  -df    -h  /</em>       <strong>-du</strong>    <strong>功能：统计文件夹的大小信息</strong>   <em>示例：</em>   *hadoop  fs  -du    -s  -h /aaa/**</td>
</tr>
<tr>
<td><strong>-count</strong>            <strong>功能：统计一个指定目录下的文件节点数量</strong>   <em>示例：hadoop fs -count /aaa/</em></td>
</tr>
<tr>
<td><strong>-setrep</strong>                   <strong>功能：设置hdfs**</strong>中文件的副本数量*<em>   </em>示例：hadoop fs -setrep 3 /aaa/jdk.tar.gz*</td>
</tr>
<tr>
<td>查看dfs集群工作状态的命令 hdfs dfsadmin -report</td>
</tr>
</tbody>
</table>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/19/1.centos7上安装hadoop2.X集群/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="严亮">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="代码块工作室">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/19/1.centos7上安装hadoop2.X集群/" itemprop="url">
                  1.centos7上安装hadoop2.X集群
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-09-19 17:49:34" itemprop="dateCreated datePublished" datetime="2018-09-19T17:49:34+08:00">2018-09-19</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2018-09-20 11:54:52" itemprop="dateModified" datetime="2018-09-20T11:54:52+08:00">2018-09-20</time>
              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/大数据/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>​1.0先将虚拟机的网络模式选为NAT</p>
<pre><code>1.1修改主机名
    vi /etc/hostname

1.2修改IP
        vi /etc/sysconfig/network-scripts/ifcfg-ens33

        TYPE=Ethernet
        PROXY_METHOD=none
        BROWSER_ONLY=no
        BOOTPROTO=static   ###
        DEFROUTE=yes
        IPV4_FAILURE_FATAL=no
        IPV6INIT=yes
        IPV6_AUTOCONF=yes
        IPV6_DEFROUTE=yes
        IPV6_FAILURE_FATAL=no
        IPV6_ADDR_GEN_MODE=stable-privacy
        NAME=ens33
        UUID=43e8ca15-c8b3-41ae-b99c-e9587c763cad
        DEVICE=ens33
        GATEWAY=192.168.100.2  ###
        NETMASK=255.255.255.0 ###
        IPADDR=192.168.100.10  ###
        DNS1=114.114.114.114 ###
        DNS2=8.8.8.8   ###
        DNS3=192.168.100.1  ###
        ONBOOT=yes         ###
</code></pre><p>​<br>​    1.3修改主机名和IP的映射关系<br>​        vim /etc/hosts<br>​<br>​        192.168.100.10 min1<br>​        192.168.100.11 min2<br>​        192.168.100.12 min3<br>​        192.168.100.13 min4<br>​<br>​    1.4关闭防火墙<br>​        systemctl stop firewalld<br>​        systemctl disable firewalld<br>​    1.5重启Linux<br>​        reboot</p>
<p>2.安装JDK</p>
<pre><code>2.1上传alt+p 后出现sftp窗口，然后put d:\xxx\yy\ll\jdk-8u181-linux-x64.tar.gz
2.2解压jdk
        #解压
        tar -zxvf jdk-8u181-linux-x64.tar.gz

2.3将java添加到环境变量中
    vim /etc/profile
    #在文件最后添加
    export JAVA_HOME=/opt/jdk1.8
    export PATH=$PATH:$JAVA_HOME/bin

    #刷新配置
    source /etc/profile
</code></pre><p>3.安装hadoop2.6.4<br>​    先上传hadoop的安装包到服务器上去/opt<br>​    注意：hadoop2.x的配置文件$HADOOP_HOME/etc/hadoop<br>​    伪分布式需要修改5个配置文件<br>3.1配置hadoop</p>
<pre><code>第一个：hadoop-env.sh
        vim hadoop-env.sh
        #第25行
        export JAVA_HOME=/opt/jdk1.8

第二个：core-site.xml
&lt;!-- 指定HADOOP所使用的文件系统schema（URI），HDFS的老大（NameNode）的地址 --&gt;
    &lt;property&gt;
        &lt;name&gt;fs.defaultFS&lt;/name&gt;
        &lt;value&gt;hdfs://min1:9000&lt;/value&gt;
    &lt;/property&gt;
    &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt;
    &lt;property&gt;
        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
        &lt;value&gt;/opt/hadoop-2.6.4/tmp&lt;/value&gt;
&lt;/property&gt;

第三个：hdfs-site.xml   
    &lt;!-- 指定HDFS副本的数量 --&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.replication&lt;/name&gt;
        &lt;value&gt;2&lt;/value&gt;
    &lt;/property&gt;

    &lt;property&gt;
        &lt;name&gt;dfs.secondary.http.address&lt;/name&gt;
        &lt;value&gt;192.168.1.152:50090&lt;/value&gt;
    &lt;/property&gt;
    &lt;!--自己选择添加----&gt;
     &lt;property&gt;
        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
        &lt;value&gt;/home/hadoop/data/name,/path2/,/path3/,nfs://&lt;/value&gt;
    &lt;/property&gt;
    &lt;!---namenode配置多个目录和datanode配置多个目录，有什么区别？----&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
        &lt;value&gt;/home/hadoop/data/data,/path2/&lt;/value&gt;
    &lt;/property&gt;

第四个：mapred-site.xml (mv mapred-site.xml.template mapred-site.xml)
    mv mapred-site.xml.template mapred-site.xml
    vim mapred-site.xml
    &lt;!-- 指定mr运行在yarn上 --&gt;
    &lt;property&gt;
        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
        &lt;value&gt;yarn&lt;/value&gt;
&lt;/property&gt;

第五个：yarn-site.xml
    &lt;!-- 指定YARN的老大（ResourceManager）的地址 --&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;
        &lt;value&gt;min1&lt;/value&gt;
&lt;/property&gt;
    &lt;!-- reducer获取数据的方式 --&gt;
&lt;property&gt;
        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
 &lt;/property&gt;




3.2将hadoop添加到环境变量

vim /etc/proflie
    export JAVA_HOME=/opt/jdk1.8
    export HADOOP_HOME=/opt/hadoop-2.6.4
    export PATH=${PATH}:${JAVA_HOME}/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

source /etc/profile

3.3格式化namenode（是对namenode进行初始化）
    hdfs namenode -format (hadoop namenode -format)

3.4启动hadoop
    先启动HDFS
    sbin/start-dfs.sh

    再启动YARN
    sbin/start-yarn.sh

3.5验证是否启动成功
    使用jps命令验证
    27408 NameNode
    28218 Jps
    27643 SecondaryNameNode
    28066 NodeManager
    27803 ResourceManager
    27512 DataNode

    http://192.168.100.10:50070 （HDFS管理界面）
    http://192.168.100.10:8088 （MR管理界面）
</code></pre><p>4.配置ssh免登陆​    </p>
<pre><code>#生成ssh免登陆密钥
#进入到我的home目录
cd ~/.ssh
ssh-keygen -t rsa （四个回车）
执行完这个命令后，会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）
将公钥拷贝到要免密登陆的目标机器上
ssh-copy-id localhost
</code></pre><ol start="5">
<li>启动命令​    <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">单一的启动方式</span><br><span class="line">hadoop-daemon.sh start namenode</span><br><span class="line">hadoop-daemon.sh start datanode</span><br><span class="line">hadoop-daemon.sh start secondarynamenode</span><br><span class="line">批量启动</span><br><span class="line">	先要去修改 vi /opt/hadoop-2.6.4/etc/hadoop/slaves</span><br><span class="line">	把需要启动的主机添加到文件中</span><br><span class="line">	min1</span><br><span class="line">    min2</span><br><span class="line">    min3</span><br><span class="line">start-dfs.sh</span><br><span class="line"></span><br><span class="line">验证</span><br><span class="line">http://192.168.100.10:50070</span><br><span class="line"></span><br><span class="line">hdfs dfs -put 本地文件路径 hdfs的文件路径</span><br><span class="line">hdfs dfs -put /etc/profile /</span><br></pre></td></tr></table></figure>
</li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/16/鸡汤/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="严亮">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="代码块工作室">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/16/鸡汤/" itemprop="url">
                  什么是鸡汤？
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-08-16 09:33:32" itemprop="dateCreated datePublished" datetime="2018-08-16T09:33:32+08:00">2018-08-16</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2018-08-17 09:32:02" itemprop="dateModified" datetime="2018-08-17T09:32:02+08:00">2018-08-17</time>
              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/鸡汤/" itemprop="url" rel="index"><span itemprop="name">鸡汤</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<p>学习分什么高低贵贱，从零好好开始。 2018年8月16日10:54:09<br>明确的说今天我们还不足够的优秀，那就多读书。    2018年8月17日09:31:36 </p>
</blockquote>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">严亮</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">6</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">2</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">7</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          
          

          
          

          
            
          
          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">严亮</span>

  

  
</div>




  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> v3.7.1</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a class="theme-link" target="_blank" href="https://theme-next.org">NexT.Muse</a> v6.4.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    
	
    

    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.4.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.4.0"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.4.0"></script>



  



  










  





  

  

  

  

  
  

  

  

  

  

  

</body>
</html>
