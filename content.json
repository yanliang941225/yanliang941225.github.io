{"meta":{"title":"代码块工作室","subtitle":null,"description":null,"author":"严亮","url":"http://yoursite.com"},"pages":[{"title":"标签","date":"2018-08-15T08:46:23.000Z","updated":"2018-08-15T08:53:51.897Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""},{"title":"分类","date":"2018-08-15T08:46:29.000Z","updated":"2018-08-15T08:54:02.961Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"2.hdfs-shell操作.md","slug":"2.hdfs-shell操作","date":"2018-09-20T02:18:32.477Z","updated":"2018-09-20T02:36:45.741Z","comments":true,"path":"2018/09/20/2.hdfs-shell操作/","link":"","permalink":"http://yoursite.com/2018/09/20/2.hdfs-shell操作/","excerpt":"","text":"[toc] 1.hdfs命令行客户端HDFS提供shell命令行客户端，使用方法如下： 可以使用一下两种形式： 12hadoop fs -… &lt;args&gt;hdfs dfs -… &lt;args&gt; 2.命令行客户端支持的命令参数123456789101112131415161718192021222324252627282930313233343536[-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;][-cat [-ignoreCrc] &lt;src&gt; ...][-checksum &lt;src&gt; ...][-chgrp [-R] GROUP PATH...][-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...][-chown [-R] [OWNER][:[GROUP]] PATH...][-copyFromLocal [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;][-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;][-count [-q] &lt;path&gt; ...][-cp [-f] [-p] &lt;src&gt; ... &lt;dst&gt;][-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]][-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;][-df [-h] [&lt;path&gt; ...]][-du [-s] [-h] &lt;path&gt; ...][-expunge][-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;][-getfacl [-R] &lt;path&gt;][-getmerge [-nl] &lt;src&gt; &lt;localdst&gt;][-help [cmd ...]][-ls [-d] [-h] [-R] [&lt;path&gt; ...]][-mkdir [-p] &lt;path&gt; ...][-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;][-moveToLocal &lt;src&gt; &lt;localdst&gt;][-mv &lt;src&gt; ... &lt;dst&gt;][-put [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;][-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;][-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...][-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...][-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]][-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...][-stat [format] &lt;path&gt; ...][-tail [-f] &lt;file&gt;][-test -[defsz] &lt;path&gt;][-text [-ignoreCrc] &lt;src&gt; ...][-touchz &lt;path&gt; ...][-usage [cmd ...]] 3.常用命令参数介绍 -help 功能：输出这个命令参数手册 -ls 功能：显示目录信息 示例： hadoop fs -ls hdfs://hadoop-server01:9000/ 备注：这些参数中，所有的hdfs**路径都可以简写 –&gt;hadoop fs -ls / 等同于上一条命令的效果 -mkdir 功能：在hdfs**上创建目录* 示例：hadoop fs -mkdir -p /aaa/bbb/cc/dd* -moveFromLocal 功能：从本地剪切粘贴到hdfs 示例：hadoop fs - moveFromLocal /home/hadoop/a.txt /aaa/bbb/cc/dd -moveToLocal 功能：从hdfs**剪切粘贴到本地* 示例：hadoop fs - moveToLocal /aaa/bbb/cc/dd /home/hadoop/a.txt* –appendToFile 功能：追加一个文件到已经存在的文件末尾 示例：hadoop fs -appendToFile ./hello.txt hdfs://hadoop-server01:9000/hello.txt 可以简写为： Hadoop fs -appendToFile ./hello.txt /hello.txt -cat 功能：显示文件内容 示例：hadoop fs -cat /hello.txt -tail 功能：显示一个文件的末尾 示例：hadoop fs -tail /weblog/access_log.1 -text 功能：以字符形式打印一个文件的内容 示例：hadoop fs -text /weblog/access_log.1 -chgrp -chmod -chown 功能：linux**文件系统中的用法一样，对文件所属权限* 示例： hadoop fs -chmod 666 /hello.txt hadoop fs -chown someuser:somegrp /hello.txt* -copyFromLocal 功能：从本地文件系统中拷贝文件到hdfs**路径去 示例：hadoop fs -copyFromLocal ./jdk.tar.gz /aaa/ -copyToLocal 功能：从hdfs**拷贝到本地 示例：hadoop fs -copyToLocal /aaa/jdk.tar.gz -cp 功能：从hdfs**的一个路径拷贝hdfs**的另一个路径 示例： hadoop fs -cp /aaa/jdk.tar.gz /bbb/jdk.tar.gz.2 -mv 功能：在hdfs**目录中移动文件* 示例： hadoop fs -mv /aaa/jdk.tar.gz /* -get 功能：等同于copyToLocal**，就是从hdfs**下载文件到本地 示例：hadoop fs -get /aaa/jdk.tar.gz - 功能：合并下载多个文件 示例：比getmerge *如hdfs的目录 /aaa/*下有多个文件:log.1, log.2,log.3,… hadoop fs -getmerge /aaa/log.* ./log.sum -put 功能：等同于copyFromLocal 示例：hadoop fs -put /aaa/jdk.tar.gz /bbb/jdk.tar.gz.2 -rm 功能：删除文件或文件夹 示例：hadoop fs -rm -r /aaa/bbb/ -rmdir 功能：删除空目录 示例：hadoop fs -rmdir /aaa/bbb/ccc -df 功能：统计文件系统的可用空间信息 示例：hadoop fs -df -h / -du 功能：统计文件夹的大小信息 示例： *hadoop fs -du -s -h /aaa/** -count 功能：统计一个指定目录下的文件节点数量 示例：hadoop fs -count /aaa/ -setrep 功能：设置hdfs**中文件的副本数量* 示例：hadoop fs -setrep 3 /aaa/jdk.tar.gz* 查看dfs集群工作状态的命令 hdfs dfsadmin -report","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/大数据/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/tags/大数据/"},{"name":"hdfs","slug":"hdfs","permalink":"http://yoursite.com/tags/hdfs/"}]},{"title":"3.hdfs-javaAPI","slug":"3.hdfs-javaAPI","date":"2018-09-20T02:18:32.477Z","updated":"2018-09-20T03:48:23.279Z","comments":true,"path":"2018/09/20/3.hdfs-javaAPI/","link":"","permalink":"http://yoursite.com/2018/09/20/3.hdfs-javaAPI/","excerpt":"","text":"[TOC] 1.java操作hdfs环境搭建1.1创建mvn项目添加依赖12345678910111213141516171819202122232425&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.6.4&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.6.4&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.6.4&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;jdk.tools&lt;/groupId&gt; &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt; &lt;version&gt;1.6&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;$&#123;JAVA_HOME&#125;/lib/tools.jar&lt;/systemPath&gt;&lt;/dependency&gt; 1.2测试1234567891011// 获取/路径下面的所有文件Configuration configuration = new Configuration();configuration.set(\"fs.defaultFS\", \"hdfs://min1:9000\");FileSystem fs = FileSystem.get(configuration);RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path(\"/\"), true);while(listFiles.hasNext()) &#123; LocatedFileStatus fileStatus = listFiles.next(); String name = fileStatus.getPath().getName(); //打印文件名 System.out.println(name);&#125;fs.close(); 1.3结果每个人的结果不同。本人的下面有三个文件： 123linux.logwindows.logprofile 2.hdfs客户端权限身份伪造的问题为什么需要权限身份伪造？ 123// 执行下面代码，需求是把本地文件上传到hdfs的/路径下fs.copyFromLocalFile(new Path(&quot;d://a.txt&quot;), new Path(&quot;/&quot;));fs.close(); 不幸的是报错了，说Administrator这个用户没有/的权限，/路径的用户是root组是supergroup，所有我们现在需要去伪造用户成root. 12org.apache.hadoop.security.AccessControlException: Permission denied: user=Administrator, access=WRITE, inode=&quot;/&quot;:root:supergroup:drwxr-xr-x at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkFsPermission(FSPermissionChecker.java:271) 处理方式： 123456789101112131415方式一： Configuration configuration = new Configuration(); configuration.set(&quot;fs.defaultFS&quot;, &quot;hdfs://min1:9000&quot;); try &#123; System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;root&quot;); fs = FileSystem.get(configuration); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; 注意：System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;root&quot;);只能放在fs = FileSystem.get(configuration);前面，不然你就彩蛋了。方式二： Configuration configuration = new Configuration(); fs = FileSystem.get(new URI(&quot;hdfs://min1:9000&quot;), configuration, &quot;root&quot;);方式三：百度一下 3.windows平台下开发hadoop需要注意的细节运行下面代码把hdfs中文件拷贝到windows下。下载功能 12fs.copyToLocalFile(new Path(&quot;/a.txt&quot;), new Path(&quot;d:/&quot;));fs.close(); 我去又报错 123456789101112131415161718192021Exception in thread &quot;main&quot; java.lang.NullPointerException at java.lang.ProcessBuilder.start(ProcessBuilder.java:1012) at org.apache.hadoop.util.Shell.runCommand(Shell.java:482) at org.apache.hadoop.util.Shell.run(Shell.java:455) at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715) at org.apache.hadoop.util.Shell.execCommand(Shell.java:808) at org.apache.hadoop.util.Shell.execCommand(Shell.java:791) at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:659) at org.apache.hadoop.fs.FilterFileSystem.setPermission(FilterFileSystem.java:490) at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:462) at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:428) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:786) at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:365) at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:338) at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289) at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:1970) at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:1939) at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:1915) at cn.mytest.hdfs.TestHDFS.main(TestHDFS.java:30) 原因： 1hadoop要与windows打交道，需要通过和windows的相关的类库进行交互操作文件。而本地不存在hadoop交互的类库。 建议： 123456789方式一：建议在linux下进行hadoop应用的开发，不会存在兼容性问题。如在window上做客户端应用开发，需要设置以下环境：A、用给的windows平台下编译的hadoop安装包解压一份到windows的任意一个目录下B、在window系统中配置HADOOP_HOME指向你解压的安装包目录C、在windows系统的path变量中加入HADOOP_HOME的bin目录方式二:采用java自己的io直接写入文件fs.copyToLocalFile(false, new Path(&quot;/a.txt&quot;), new Path(&quot;d:/&quot;), true); 4.hdfs客户端文件上传12345678910111213141516171819202122232425262728293031323334353637FileSystem fs = null; @Before public void init() throws Exception &#123; // 构造一个配置参数对象，设置一个参数：我们要访问的hdfs的URI // 从而FileSystem.get()方法就知道应该是去构造一个访问hdfs文件系统的客户端，以及hdfs的访问地址 // new Configuration();的时候，它就会去加载jar包中的hdfs-default.xml // 然后再加载classpath下的hdfs-site.xml Configuration conf = new Configuration(); // conf.set(\"fs.defaultFS\", \"hdfs://mini1:9000\"); //参数优先级： 1、客户端代码中设置的值 2、classpath下的用户自定义配置文件 3、然后是服务器的默认配置 /*conf.set(\"dfs.replication\", \"2\"); conf.set(\"dfs.block.size\", \"64m\");*/ // 获取一个hdfs的访问客户端，根据参数，这个实例应该是DistributedFileSystem的实例 // fs = FileSystem.get(conf); // 如果这样去获取，那conf里面就可以不要配\"fs.defaultFS\"参数，而且，这个客户端的身份标识已经是root用户 fs = FileSystem.get(new URI(\"hdfs://mini1:9000\"), conf, \"root\"); &#125;方式一： public void testAddFileToHdfs() throws Exception &#123; Path src = new Path(\"d://a.txt\"); Path dst = new Path(\"/\"); fs.copyFromLocalFile(src, dst); fs.close(); &#125;方式二流: FileInputStream in = new FileInputStream(\"d://a.txt\"); Path path = new Path(\"/aa.txt\"); FSDataOutputStream out = fs.create(path); IOUtils.copy(in, out); fs.close(); 5.hdfs客户端文件下载12345678方式一: fs.copyToLocalFile(false, new Path(\"/a.txt\"), new Path(\"d:/\"), true); fs.close();方式二流： FSDataInputStream in = fs.open(new Path(\"/a.txt\")); FileOutputStream out = new FileOutputStream(new File(\"d:/a.txt\")); IOUtils.copy(in, out); fs.close(); 6.hdfs客户端目录操作、查看文件夹以及文件信息123456789101112131415161718//查看目录信息，只显示文件RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path(\"/\"), true);while (listFiles.hasNext()) &#123; LocatedFileStatus fileStatus = listFiles.next(); System.out.println(fileStatus.getPath().getName()); System.out.println(fileStatus.getBlockSize()); System.out.println(fileStatus.getPermission()); System.out.println(fileStatus.getLen()); BlockLocation[] blockLocations = fileStatus.getBlockLocations(); for (BlockLocation bl : blockLocations) &#123; System.out.println(\"block-length:\" + bl.getLength() + \"--\" + \"block-offset:\" + bl.getOffset()); String[] hosts = bl.getHosts(); for (String host : hosts) &#123; System.out.println(host); &#125; &#125; System.out.println(\"--------------分割线--------------\");&#125; 123456789101112// 查看文件及文件夹信息FileStatus[] listStatus = fs.listStatus(new Path(\"/\"));String flag = \"\";for (FileStatus fstatus : listStatus) &#123; if (fstatus.isFile()) &#123; flag = \"文件\"; &#125; else &#123; flag = \"文件夹\"; &#125; System.out.println(flag + fstatus.getPath().getName()); System.out.println(fstatus.getPermission());&#125; 1234567891011// hdfs支持随机定位进行文件读取，而且可以方便地读取指定长度 用于上层分布式运算框架并发处理数据// 先获取一个文件的输入流----针对hdfs上的FSDataInputStream in = fs.open(new Path(\"/a.txt\"));// 可以将流的起始偏移量进行自定义in.seek(22);// 再构造一个文件的输出流----针对本地的FileOutputStream out = new FileOutputStream(new File(\"d:/a.txt\"));org.apache.hadoop.io.IOUtils.copyBytes(in, out, 19L, true);","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/大数据/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/tags/大数据/"},{"name":"hdfs","slug":"hdfs","permalink":"http://yoursite.com/tags/hdfs/"},{"name":"java","slug":"java","permalink":"http://yoursite.com/tags/java/"}]},{"title":"1.centos7上安装hadoop2.X集群","slug":"1.centos7上安装hadoop2.X集群","date":"2018-09-19T09:49:34.641Z","updated":"2018-09-19T09:57:02.854Z","comments":true,"path":"2018/09/19/1.centos7上安装hadoop2.X集群/","link":"","permalink":"http://yoursite.com/2018/09/19/1.centos7上安装hadoop2.X集群/","excerpt":"","text":"​1.0先将虚拟机的网络模式选为NAT 1.1修改主机名 vi /etc/hostname 1.2修改IP vi /etc/sysconfig/network-scripts/ifcfg-ens33 TYPE=Ethernet PROXY_METHOD=none BROWSER_ONLY=no BOOTPROTO=static ### DEFROUTE=yes IPV4_FAILURE_FATAL=no IPV6INIT=yes IPV6_AUTOCONF=yes IPV6_DEFROUTE=yes IPV6_FAILURE_FATAL=no IPV6_ADDR_GEN_MODE=stable-privacy NAME=ens33 UUID=43e8ca15-c8b3-41ae-b99c-e9587c763cad DEVICE=ens33 GATEWAY=192.168.100.2 ### NETMASK=255.255.255.0 ### IPADDR=192.168.100.10 ### DNS1=114.114.114.114 ### DNS2=8.8.8.8 ### DNS3=192.168.100.1 ### ONBOOT=yes ### ​​ 1.3修改主机名和IP的映射关系​ vim /etc/hosts​​ 192.168.100.10 min1​ 192.168.100.11 min2​ 192.168.100.12 min3​ 192.168.100.13 min4​​ 1.4关闭防火墙​ systemctl stop firewalld​ systemctl disable firewalld​ 1.5重启Linux​ reboot 2.安装JDK 2.1上传alt+p 后出现sftp窗口，然后put d:\\xxx\\yy\\ll\\jdk-8u181-linux-x64.tar.gz 2.2解压jdk #解压 tar -zxvf jdk-8u181-linux-x64.tar.gz 2.3将java添加到环境变量中 vim /etc/profile #在文件最后添加 export JAVA_HOME=/opt/jdk1.8 export PATH=$PATH:$JAVA_HOME/bin #刷新配置 source /etc/profile 3.安装hadoop2.6.4​ 先上传hadoop的安装包到服务器上去/opt​ 注意：hadoop2.x的配置文件$HADOOP_HOME/etc/hadoop​ 伪分布式需要修改5个配置文件3.1配置hadoop 第一个：hadoop-env.sh vim hadoop-env.sh #第25行 export JAVA_HOME=/opt/jdk1.8 第二个：core-site.xml &lt;!-- 指定HADOOP所使用的文件系统schema（URI），HDFS的老大（NameNode）的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://min1:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/hadoop-2.6.4/tmp&lt;/value&gt; &lt;/property&gt; 第三个：hdfs-site.xml &lt;!-- 指定HDFS副本的数量 --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.secondary.http.address&lt;/name&gt; &lt;value&gt;192.168.1.152:50090&lt;/value&gt; &lt;/property&gt; &lt;!--自己选择添加----&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data/name,/path2/,/path3/,nfs://&lt;/value&gt; &lt;/property&gt; &lt;!---namenode配置多个目录和datanode配置多个目录，有什么区别？----&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data/data,/path2/&lt;/value&gt; &lt;/property&gt; 第四个：mapred-site.xml (mv mapred-site.xml.template mapred-site.xml) mv mapred-site.xml.template mapred-site.xml vim mapred-site.xml &lt;!-- 指定mr运行在yarn上 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; 第五个：yarn-site.xml &lt;!-- 指定YARN的老大（ResourceManager）的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;min1&lt;/value&gt; &lt;/property&gt; &lt;!-- reducer获取数据的方式 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; 3.2将hadoop添加到环境变量 vim /etc/proflie export JAVA_HOME=/opt/jdk1.8 export HADOOP_HOME=/opt/hadoop-2.6.4 export PATH=${PATH}:${JAVA_HOME}/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin source /etc/profile 3.3格式化namenode（是对namenode进行初始化） hdfs namenode -format (hadoop namenode -format) 3.4启动hadoop 先启动HDFS sbin/start-dfs.sh 再启动YARN sbin/start-yarn.sh 3.5验证是否启动成功 使用jps命令验证 27408 NameNode 28218 Jps 27643 SecondaryNameNode 28066 NodeManager 27803 ResourceManager 27512 DataNode http://192.168.100.10:50070 （HDFS管理界面） http://192.168.100.10:8088 （MR管理界面） 4.配置ssh免登陆​ #生成ssh免登陆密钥 #进入到我的home目录 cd ~/.ssh ssh-keygen -t rsa （四个回车） 执行完这个命令后，会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥） 将公钥拷贝到要免密登陆的目标机器上 ssh-copy-id localhost 启动命令​ 1234567891011121314151617单一的启动方式hadoop-daemon.sh start namenodehadoop-daemon.sh start datanodehadoop-daemon.sh start secondarynamenode批量启动 先要去修改 vi /opt/hadoop-2.6.4/etc/hadoop/slaves 把需要启动的主机添加到文件中 min1 min2 min3start-dfs.sh验证http://192.168.100.10:50070hdfs dfs -put 本地文件路径 hdfs的文件路径hdfs dfs -put /etc/profile /","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/大数据/"}],"tags":[{"name":"hadoop安装","slug":"hadoop安装","permalink":"http://yoursite.com/tags/hadoop安装/"}]},{"title":"什么是鸡汤？","slug":"鸡汤","date":"2018-08-16T01:33:32.442Z","updated":"2018-08-17T01:32:02.900Z","comments":true,"path":"2018/08/16/鸡汤/","link":"","permalink":"http://yoursite.com/2018/08/16/鸡汤/","excerpt":"","text":"学习分什么高低贵贱，从零好好开始。 2018年8月16日10:54:09明确的说今天我们还不足够的优秀，那就多读书。 2018年8月17日09:31:36","categories":[{"name":"鸡汤","slug":"鸡汤","permalink":"http://yoursite.com/categories/鸡汤/"}],"tags":[{"name":"鸡汤","slug":"鸡汤","permalink":"http://yoursite.com/tags/鸡汤/"}]}]}