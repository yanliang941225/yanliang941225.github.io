{"meta":{"title":"代码块工作室","subtitle":null,"description":null,"author":"严亮","url":"http://yoursite.com"},"pages":[{"title":"标签","date":"2018-08-15T08:46:23.000Z","updated":"2018-08-15T08:53:51.897Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""},{"title":"分类","date":"2018-08-15T08:46:29.000Z","updated":"2018-08-15T08:54:02.961Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"3.hdfs-javaAPI","slug":"3.hdfs-javaAPI","date":"2018-09-20T02:18:32.477Z","updated":"2018-09-20T02:39:55.822Z","comments":true,"path":"2018/09/20/3.hdfs-javaAPI/","link":"","permalink":"http://yoursite.com/2018/09/20/3.hdfs-javaAPI/","excerpt":"","text":"[TOC] 1.java操作hdfs环境搭建2.hdfs客户端权限身份伪造的问题3.windows平台下开发hadoop需要注意的细节4.hdfs客户端文件上传5.hdfs客户端文件下载6.hdfs客户端目录操作、查看文件夹以及文件信息","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/大数据/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/tags/大数据/"},{"name":"hdfs","slug":"hdfs","permalink":"http://yoursite.com/tags/hdfs/"},{"name":"java","slug":"java","permalink":"http://yoursite.com/tags/java/"}]},{"title":"2.hdfs-shell操作.md","slug":"2.hdfs-shell操作","date":"2018-09-20T02:18:32.477Z","updated":"2018-09-20T02:36:45.741Z","comments":true,"path":"2018/09/20/2.hdfs-shell操作/","link":"","permalink":"http://yoursite.com/2018/09/20/2.hdfs-shell操作/","excerpt":"","text":"[toc] 1.hdfs命令行客户端HDFS提供shell命令行客户端，使用方法如下： 可以使用一下两种形式： 12hadoop fs -… &lt;args&gt;hdfs dfs -… &lt;args&gt; 2.命令行客户端支持的命令参数123456789101112131415161718192021222324252627282930313233343536[-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;][-cat [-ignoreCrc] &lt;src&gt; ...][-checksum &lt;src&gt; ...][-chgrp [-R] GROUP PATH...][-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...][-chown [-R] [OWNER][:[GROUP]] PATH...][-copyFromLocal [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;][-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;][-count [-q] &lt;path&gt; ...][-cp [-f] [-p] &lt;src&gt; ... &lt;dst&gt;][-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]][-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;][-df [-h] [&lt;path&gt; ...]][-du [-s] [-h] &lt;path&gt; ...][-expunge][-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;][-getfacl [-R] &lt;path&gt;][-getmerge [-nl] &lt;src&gt; &lt;localdst&gt;][-help [cmd ...]][-ls [-d] [-h] [-R] [&lt;path&gt; ...]][-mkdir [-p] &lt;path&gt; ...][-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;][-moveToLocal &lt;src&gt; &lt;localdst&gt;][-mv &lt;src&gt; ... &lt;dst&gt;][-put [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;][-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;][-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...][-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...][-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]][-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...][-stat [format] &lt;path&gt; ...][-tail [-f] &lt;file&gt;][-test -[defsz] &lt;path&gt;][-text [-ignoreCrc] &lt;src&gt; ...][-touchz &lt;path&gt; ...][-usage [cmd ...]] 3.常用命令参数介绍 -help 功能：输出这个命令参数手册 -ls 功能：显示目录信息 示例： hadoop fs -ls hdfs://hadoop-server01:9000/ 备注：这些参数中，所有的hdfs**路径都可以简写 –&gt;hadoop fs -ls / 等同于上一条命令的效果 -mkdir 功能：在hdfs**上创建目录* 示例：hadoop fs -mkdir -p /aaa/bbb/cc/dd* -moveFromLocal 功能：从本地剪切粘贴到hdfs 示例：hadoop fs - moveFromLocal /home/hadoop/a.txt /aaa/bbb/cc/dd -moveToLocal 功能：从hdfs**剪切粘贴到本地* 示例：hadoop fs - moveToLocal /aaa/bbb/cc/dd /home/hadoop/a.txt* –appendToFile 功能：追加一个文件到已经存在的文件末尾 示例：hadoop fs -appendToFile ./hello.txt hdfs://hadoop-server01:9000/hello.txt 可以简写为： Hadoop fs -appendToFile ./hello.txt /hello.txt -cat 功能：显示文件内容 示例：hadoop fs -cat /hello.txt -tail 功能：显示一个文件的末尾 示例：hadoop fs -tail /weblog/access_log.1 -text 功能：以字符形式打印一个文件的内容 示例：hadoop fs -text /weblog/access_log.1 -chgrp -chmod -chown 功能：linux**文件系统中的用法一样，对文件所属权限* 示例： hadoop fs -chmod 666 /hello.txt hadoop fs -chown someuser:somegrp /hello.txt* -copyFromLocal 功能：从本地文件系统中拷贝文件到hdfs**路径去 示例：hadoop fs -copyFromLocal ./jdk.tar.gz /aaa/ -copyToLocal 功能：从hdfs**拷贝到本地 示例：hadoop fs -copyToLocal /aaa/jdk.tar.gz -cp 功能：从hdfs**的一个路径拷贝hdfs**的另一个路径 示例： hadoop fs -cp /aaa/jdk.tar.gz /bbb/jdk.tar.gz.2 -mv 功能：在hdfs**目录中移动文件* 示例： hadoop fs -mv /aaa/jdk.tar.gz /* -get 功能：等同于copyToLocal**，就是从hdfs**下载文件到本地 示例：hadoop fs -get /aaa/jdk.tar.gz - 功能：合并下载多个文件 示例：比getmerge *如hdfs的目录 /aaa/*下有多个文件:log.1, log.2,log.3,… hadoop fs -getmerge /aaa/log.* ./log.sum -put 功能：等同于copyFromLocal 示例：hadoop fs -put /aaa/jdk.tar.gz /bbb/jdk.tar.gz.2 -rm 功能：删除文件或文件夹 示例：hadoop fs -rm -r /aaa/bbb/ -rmdir 功能：删除空目录 示例：hadoop fs -rmdir /aaa/bbb/ccc -df 功能：统计文件系统的可用空间信息 示例：hadoop fs -df -h / -du 功能：统计文件夹的大小信息 示例： *hadoop fs -du -s -h /aaa/** -count 功能：统计一个指定目录下的文件节点数量 示例：hadoop fs -count /aaa/ -setrep 功能：设置hdfs**中文件的副本数量* 示例：hadoop fs -setrep 3 /aaa/jdk.tar.gz* 查看dfs集群工作状态的命令 hdfs dfsadmin -report","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/大数据/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/tags/大数据/"},{"name":"hdfs","slug":"hdfs","permalink":"http://yoursite.com/tags/hdfs/"}]},{"title":"1.centos7上安装hadoop2.X集群","slug":"1.centos7上安装hadoop2.X集群","date":"2018-09-19T09:49:34.641Z","updated":"2018-09-19T09:57:02.854Z","comments":true,"path":"2018/09/19/1.centos7上安装hadoop2.X集群/","link":"","permalink":"http://yoursite.com/2018/09/19/1.centos7上安装hadoop2.X集群/","excerpt":"","text":"​1.0先将虚拟机的网络模式选为NAT 1.1修改主机名 vi /etc/hostname 1.2修改IP vi /etc/sysconfig/network-scripts/ifcfg-ens33 TYPE=Ethernet PROXY_METHOD=none BROWSER_ONLY=no BOOTPROTO=static ### DEFROUTE=yes IPV4_FAILURE_FATAL=no IPV6INIT=yes IPV6_AUTOCONF=yes IPV6_DEFROUTE=yes IPV6_FAILURE_FATAL=no IPV6_ADDR_GEN_MODE=stable-privacy NAME=ens33 UUID=43e8ca15-c8b3-41ae-b99c-e9587c763cad DEVICE=ens33 GATEWAY=192.168.100.2 ### NETMASK=255.255.255.0 ### IPADDR=192.168.100.10 ### DNS1=114.114.114.114 ### DNS2=8.8.8.8 ### DNS3=192.168.100.1 ### ONBOOT=yes ### ​​ 1.3修改主机名和IP的映射关系​ vim /etc/hosts​​ 192.168.100.10 min1​ 192.168.100.11 min2​ 192.168.100.12 min3​ 192.168.100.13 min4​​ 1.4关闭防火墙​ systemctl stop firewalld​ systemctl disable firewalld​ 1.5重启Linux​ reboot 2.安装JDK 2.1上传alt+p 后出现sftp窗口，然后put d:\\xxx\\yy\\ll\\jdk-8u181-linux-x64.tar.gz 2.2解压jdk #解压 tar -zxvf jdk-8u181-linux-x64.tar.gz 2.3将java添加到环境变量中 vim /etc/profile #在文件最后添加 export JAVA_HOME=/opt/jdk1.8 export PATH=$PATH:$JAVA_HOME/bin #刷新配置 source /etc/profile 3.安装hadoop2.6.4​ 先上传hadoop的安装包到服务器上去/opt​ 注意：hadoop2.x的配置文件$HADOOP_HOME/etc/hadoop​ 伪分布式需要修改5个配置文件3.1配置hadoop 第一个：hadoop-env.sh vim hadoop-env.sh #第25行 export JAVA_HOME=/opt/jdk1.8 第二个：core-site.xml &lt;!-- 指定HADOOP所使用的文件系统schema（URI），HDFS的老大（NameNode）的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://min1:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/hadoop-2.6.4/tmp&lt;/value&gt; &lt;/property&gt; 第三个：hdfs-site.xml &lt;!-- 指定HDFS副本的数量 --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.secondary.http.address&lt;/name&gt; &lt;value&gt;192.168.1.152:50090&lt;/value&gt; &lt;/property&gt; &lt;!--自己选择添加----&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data/name,/path2/,/path3/,nfs://&lt;/value&gt; &lt;/property&gt; &lt;!---namenode配置多个目录和datanode配置多个目录，有什么区别？----&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data/data,/path2/&lt;/value&gt; &lt;/property&gt; 第四个：mapred-site.xml (mv mapred-site.xml.template mapred-site.xml) mv mapred-site.xml.template mapred-site.xml vim mapred-site.xml &lt;!-- 指定mr运行在yarn上 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; 第五个：yarn-site.xml &lt;!-- 指定YARN的老大（ResourceManager）的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;min1&lt;/value&gt; &lt;/property&gt; &lt;!-- reducer获取数据的方式 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; 3.2将hadoop添加到环境变量 vim /etc/proflie export JAVA_HOME=/opt/jdk1.8 export HADOOP_HOME=/opt/hadoop-2.6.4 export PATH=${PATH}:${JAVA_HOME}/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin source /etc/profile 3.3格式化namenode（是对namenode进行初始化） hdfs namenode -format (hadoop namenode -format) 3.4启动hadoop 先启动HDFS sbin/start-dfs.sh 再启动YARN sbin/start-yarn.sh 3.5验证是否启动成功 使用jps命令验证 27408 NameNode 28218 Jps 27643 SecondaryNameNode 28066 NodeManager 27803 ResourceManager 27512 DataNode http://192.168.100.10:50070 （HDFS管理界面） http://192.168.100.10:8088 （MR管理界面） 4.配置ssh免登陆​ #生成ssh免登陆密钥 #进入到我的home目录 cd ~/.ssh ssh-keygen -t rsa （四个回车） 执行完这个命令后，会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥） 将公钥拷贝到要免密登陆的目标机器上 ssh-copy-id localhost 启动命令​ 1234567891011121314151617单一的启动方式hadoop-daemon.sh start namenodehadoop-daemon.sh start datanodehadoop-daemon.sh start secondarynamenode批量启动 先要去修改 vi /opt/hadoop-2.6.4/etc/hadoop/slaves 把需要启动的主机添加到文件中 min1 min2 min3start-dfs.sh验证http://192.168.100.10:50070hdfs dfs -put 本地文件路径 hdfs的文件路径hdfs dfs -put /etc/profile /","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/大数据/"}],"tags":[{"name":"hadoop安装","slug":"hadoop安装","permalink":"http://yoursite.com/tags/hadoop安装/"}]},{"title":"什么是鸡汤？","slug":"鸡汤","date":"2018-08-16T01:33:32.442Z","updated":"2018-08-17T01:32:02.900Z","comments":true,"path":"2018/08/16/鸡汤/","link":"","permalink":"http://yoursite.com/2018/08/16/鸡汤/","excerpt":"","text":"学习分什么高低贵贱，从零好好开始。 2018年8月16日10:54:09明确的说今天我们还不足够的优秀，那就多读书。 2018年8月17日09:31:36","categories":[{"name":"鸡汤","slug":"鸡汤","permalink":"http://yoursite.com/categories/鸡汤/"}],"tags":[{"name":"鸡汤","slug":"鸡汤","permalink":"http://yoursite.com/tags/鸡汤/"}]}]}