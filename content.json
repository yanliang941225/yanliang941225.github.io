{"meta":{"title":"代码块工作室","subtitle":null,"description":null,"author":"严亮","url":"http://yoursite.com"},"pages":[{"title":"分类","date":"2018-08-15T08:46:29.000Z","updated":"2018-08-15T08:54:02.961Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2018-08-15T08:46:23.000Z","updated":"2018-08-15T08:53:51.897Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"1.centos7上安装hadoop2.X集群","slug":"1.centos7上安装hadoop2.X集群","date":"2018-09-19T09:49:34.641Z","updated":"2018-09-19T09:57:02.854Z","comments":true,"path":"2018/09/19/1.centos7上安装hadoop2.X集群/","link":"","permalink":"http://yoursite.com/2018/09/19/1.centos7上安装hadoop2.X集群/","excerpt":"","text":"​1.0先将虚拟机的网络模式选为NAT 1.1修改主机名 vi /etc/hostname 1.2修改IP vi /etc/sysconfig/network-scripts/ifcfg-ens33 TYPE=Ethernet PROXY_METHOD=none BROWSER_ONLY=no BOOTPROTO=static ### DEFROUTE=yes IPV4_FAILURE_FATAL=no IPV6INIT=yes IPV6_AUTOCONF=yes IPV6_DEFROUTE=yes IPV6_FAILURE_FATAL=no IPV6_ADDR_GEN_MODE=stable-privacy NAME=ens33 UUID=43e8ca15-c8b3-41ae-b99c-e9587c763cad DEVICE=ens33 GATEWAY=192.168.100.2 ### NETMASK=255.255.255.0 ### IPADDR=192.168.100.10 ### DNS1=114.114.114.114 ### DNS2=8.8.8.8 ### DNS3=192.168.100.1 ### ONBOOT=yes ### ​​ 1.3修改主机名和IP的映射关系​ vim /etc/hosts​​ 192.168.100.10 min1​ 192.168.100.11 min2​ 192.168.100.12 min3​ 192.168.100.13 min4​​ 1.4关闭防火墙​ systemctl stop firewalld​ systemctl disable firewalld​ 1.5重启Linux​ reboot 2.安装JDK 2.1上传alt+p 后出现sftp窗口，然后put d:\\xxx\\yy\\ll\\jdk-8u181-linux-x64.tar.gz 2.2解压jdk #解压 tar -zxvf jdk-8u181-linux-x64.tar.gz 2.3将java添加到环境变量中 vim /etc/profile #在文件最后添加 export JAVA_HOME=/opt/jdk1.8 export PATH=$PATH:$JAVA_HOME/bin #刷新配置 source /etc/profile 3.安装hadoop2.6.4​ 先上传hadoop的安装包到服务器上去/opt​ 注意：hadoop2.x的配置文件$HADOOP_HOME/etc/hadoop​ 伪分布式需要修改5个配置文件3.1配置hadoop 第一个：hadoop-env.sh vim hadoop-env.sh #第25行 export JAVA_HOME=/opt/jdk1.8 第二个：core-site.xml &lt;!-- 指定HADOOP所使用的文件系统schema（URI），HDFS的老大（NameNode）的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://min1:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/hadoop-2.6.4/tmp&lt;/value&gt; &lt;/property&gt; 第三个：hdfs-site.xml &lt;!-- 指定HDFS副本的数量 --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.secondary.http.address&lt;/name&gt; &lt;value&gt;192.168.1.152:50090&lt;/value&gt; &lt;/property&gt; &lt;!--自己选择添加----&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data/name,/path2/,/path3/,nfs://&lt;/value&gt; &lt;/property&gt; &lt;!---namenode配置多个目录和datanode配置多个目录，有什么区别？----&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data/data,/path2/&lt;/value&gt; &lt;/property&gt; 第四个：mapred-site.xml (mv mapred-site.xml.template mapred-site.xml) mv mapred-site.xml.template mapred-site.xml vim mapred-site.xml &lt;!-- 指定mr运行在yarn上 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; 第五个：yarn-site.xml &lt;!-- 指定YARN的老大（ResourceManager）的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;min1&lt;/value&gt; &lt;/property&gt; &lt;!-- reducer获取数据的方式 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; 3.2将hadoop添加到环境变量 vim /etc/proflie export JAVA_HOME=/opt/jdk1.8 export HADOOP_HOME=/opt/hadoop-2.6.4 export PATH=${PATH}:${JAVA_HOME}/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin source /etc/profile 3.3格式化namenode（是对namenode进行初始化） hdfs namenode -format (hadoop namenode -format) 3.4启动hadoop 先启动HDFS sbin/start-dfs.sh 再启动YARN sbin/start-yarn.sh 3.5验证是否启动成功 使用jps命令验证 27408 NameNode 28218 Jps 27643 SecondaryNameNode 28066 NodeManager 27803 ResourceManager 27512 DataNode http://192.168.100.10:50070 （HDFS管理界面） http://192.168.100.10:8088 （MR管理界面） 4.配置ssh免登陆​ #生成ssh免登陆密钥 #进入到我的home目录 cd ~/.ssh ssh-keygen -t rsa （四个回车） 执行完这个命令后，会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥） 将公钥拷贝到要免密登陆的目标机器上 ssh-copy-id localhost 启动命令​ 1234567891011121314151617单一的启动方式hadoop-daemon.sh start namenodehadoop-daemon.sh start datanodehadoop-daemon.sh start secondarynamenode批量启动 先要去修改 vi /opt/hadoop-2.6.4/etc/hadoop/slaves 把需要启动的主机添加到文件中 min1 min2 min3start-dfs.sh验证http://192.168.100.10:50070hdfs dfs -put 本地文件路径 hdfs的文件路径hdfs dfs -put /etc/profile /","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/大数据/"}],"tags":[{"name":"hadoop安装","slug":"hadoop安装","permalink":"http://yoursite.com/tags/hadoop安装/"}]},{"title":"什么是鸡汤？","slug":"鸡汤","date":"2018-08-16T01:33:32.442Z","updated":"2018-08-17T01:32:02.900Z","comments":true,"path":"2018/08/16/鸡汤/","link":"","permalink":"http://yoursite.com/2018/08/16/鸡汤/","excerpt":"","text":"学习分什么高低贵贱，从零好好开始。 2018年8月16日10:54:09明确的说今天我们还不足够的优秀，那就多读书。 2018年8月17日09:31:36","categories":[{"name":"鸡汤","slug":"鸡汤","permalink":"http://yoursite.com/categories/鸡汤/"}],"tags":[{"name":"鸡汤","slug":"鸡汤","permalink":"http://yoursite.com/tags/鸡汤/"}]}]}